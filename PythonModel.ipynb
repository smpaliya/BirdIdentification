{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cfe1c0-1a83-44df-8dd1-327584e2c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "987ddd8d-bae1-46ad-9b66-c4d60fce0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b614ef6b-0d24-43d0-bbc8-c7d1e1c59c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('C:\\\\newBirddataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbca8d6c-d1ae-4926-9a91-3569028c96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + val_split + test_split) == 1, \"Splits must sum to 1.\"\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "\n",
    "    train_size = int(train_split * len(ds))\n",
    "    val_size = int(val_split * len(ds))\n",
    "\n",
    "    train_ds = ds.take(train_size)\n",
    "    remaining = ds.skip(train_size)\n",
    "    val_ds = remaining.take(val_size)\n",
    "    test_ds = remaining.skip(val_size)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3992fcf-6718-4673-96c1-337b42753a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11788 files belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'C:\\\\newBirddataset\\\\images',\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    image_size=(224, 224),\n",
    ")\n",
    "labels=dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06991f9b-f1b4-49f2-97ea-070519f0e269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4e0835-728c-4b46-b225-a256c421e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names from the dataset\n",
    "class_names = dataset.class_names\n",
    "#splitting in train,test and validation\n",
    "train,test,validation=get_dataset_partitions_tf(dataset)\n",
    "# Create a lookup layer to convert class names to integers\n",
    "label_lookup = tf.keras.layers.StringLookup(vocabulary=class_names, num_oov_indices=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "216731c6-8096-4332-9848-b3e7c66cd90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TakeDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02877a03-8bc4-4070-842e-66c3980a1cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the data augmentation pipeline\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal_and_vertical'),  # Randomly flip images horizontally and vertically\n",
    "    tf.keras.layers.RandomRotation(0.2),  # Randomly rotate images by 20%\n",
    "    tf.keras.layers.RandomZoom(0.2),  # Randomly zoom into images\n",
    "    tf.keras.layers.RandomContrast(0.1),  # Randomly adjust contrast\n",
    "])\n",
    "\n",
    "# Apply augmentation to the dataset\n",
    "augmented_train = train.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "\n",
    "# Now you can use augmented_dataset for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb90aa4-606e-4d97-a4a7-c02ca72d0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to integer indices\n",
    "augmented_train = augmented_train.map(lambda x, y: (x, y))\n",
    "validation = validation.map(lambda x, y: (x, y))\n",
    "test = test.map(lambda x, y: (x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56963ddd-2442-495b-b28b-b4f4634913ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['001.Black_footed_Albatross',\n",
       " '002.Laysan_Albatross',\n",
       " '003.Sooty_Albatross',\n",
       " '004.Groove_billed_Ani',\n",
       " '005.Crested_Auklet',\n",
       " '006.Least_Auklet',\n",
       " '007.Parakeet_Auklet',\n",
       " '008.Rhinoceros_Auklet',\n",
       " '009.Brewer_Blackbird',\n",
       " '010.Red_winged_Blackbird',\n",
       " '011.Rusty_Blackbird',\n",
       " '012.Yellow_headed_Blackbird',\n",
       " '013.Bobolink',\n",
       " '014.Indigo_Bunting',\n",
       " '015.Lazuli_Bunting',\n",
       " '016.Painted_Bunting',\n",
       " '017.Cardinal',\n",
       " '018.Spotted_Catbird',\n",
       " '019.Gray_Catbird',\n",
       " '020.Yellow_breasted_Chat',\n",
       " '021.Eastern_Towhee',\n",
       " '022.Chuck_will_Widow',\n",
       " '023.Brandt_Cormorant',\n",
       " '024.Red_faced_Cormorant',\n",
       " '025.Pelagic_Cormorant',\n",
       " '026.Bronzed_Cowbird',\n",
       " '027.Shiny_Cowbird',\n",
       " '028.Brown_Creeper',\n",
       " '029.American_Crow',\n",
       " '030.Fish_Crow',\n",
       " '031.Black_billed_Cuckoo',\n",
       " '032.Mangrove_Cuckoo',\n",
       " '033.Yellow_billed_Cuckoo',\n",
       " '034.Gray_crowned_Rosy_Finch',\n",
       " '035.Purple_Finch',\n",
       " '036.Northern_Flicker',\n",
       " '037.Acadian_Flycatcher',\n",
       " '038.Great_Crested_Flycatcher',\n",
       " '039.Least_Flycatcher',\n",
       " '040.Olive_sided_Flycatcher',\n",
       " '041.Scissor_tailed_Flycatcher',\n",
       " '042.Vermilion_Flycatcher',\n",
       " '043.Yellow_bellied_Flycatcher',\n",
       " '044.Frigatebird',\n",
       " '045.Northern_Fulmar',\n",
       " '046.Gadwall',\n",
       " '047.American_Goldfinch',\n",
       " '048.European_Goldfinch',\n",
       " '049.Boat_tailed_Grackle',\n",
       " '050.Eared_Grebe',\n",
       " '051.Horned_Grebe',\n",
       " '052.Pied_billed_Grebe',\n",
       " '053.Western_Grebe',\n",
       " '054.Blue_Grosbeak',\n",
       " '055.Evening_Grosbeak',\n",
       " '056.Pine_Grosbeak',\n",
       " '057.Rose_breasted_Grosbeak',\n",
       " '058.Pigeon_Guillemot',\n",
       " '059.California_Gull',\n",
       " '060.Glaucous_winged_Gull',\n",
       " '061.Heermann_Gull',\n",
       " '062.Herring_Gull',\n",
       " '063.Ivory_Gull',\n",
       " '064.Ring_billed_Gull',\n",
       " '065.Slaty_backed_Gull',\n",
       " '066.Western_Gull',\n",
       " '067.Anna_Hummingbird',\n",
       " '068.Ruby_throated_Hummingbird',\n",
       " '069.Rufous_Hummingbird',\n",
       " '070.Green_Violetear',\n",
       " '071.Long_tailed_Jaeger',\n",
       " '072.Pomarine_Jaeger',\n",
       " '073.Blue_Jay',\n",
       " '074.Florida_Jay',\n",
       " '075.Green_Jay',\n",
       " '076.Dark_eyed_Junco',\n",
       " '077.Tropical_Kingbird',\n",
       " '078.Gray_Kingbird',\n",
       " '079.Belted_Kingfisher',\n",
       " '080.Green_Kingfisher',\n",
       " '081.Pied_Kingfisher',\n",
       " '082.Ringed_Kingfisher',\n",
       " '083.White_breasted_Kingfisher',\n",
       " '084.Red_legged_Kittiwake',\n",
       " '085.Horned_Lark',\n",
       " '086.Pacific_Loon',\n",
       " '087.Mallard',\n",
       " '088.Western_Meadowlark',\n",
       " '089.Hooded_Merganser',\n",
       " '090.Red_breasted_Merganser',\n",
       " '091.Mockingbird',\n",
       " '092.Nighthawk',\n",
       " '093.Clark_Nutcracker',\n",
       " '094.White_breasted_Nuthatch',\n",
       " '095.Baltimore_Oriole',\n",
       " '096.Hooded_Oriole',\n",
       " '097.Orchard_Oriole',\n",
       " '098.Scott_Oriole',\n",
       " '099.Ovenbird',\n",
       " '100.Brown_Pelican',\n",
       " '101.White_Pelican',\n",
       " '102.Western_Wood_Pewee',\n",
       " '103.Sayornis',\n",
       " '104.American_Pipit',\n",
       " '105.Whip_poor_Will',\n",
       " '106.Horned_Puffin',\n",
       " '107.Common_Raven',\n",
       " '108.White_necked_Raven',\n",
       " '109.American_Redstart',\n",
       " '110.Geococcyx',\n",
       " '111.Loggerhead_Shrike',\n",
       " '112.Great_Grey_Shrike',\n",
       " '113.Baird_Sparrow',\n",
       " '114.Black_throated_Sparrow',\n",
       " '115.Brewer_Sparrow',\n",
       " '116.Chipping_Sparrow',\n",
       " '117.Clay_colored_Sparrow',\n",
       " '118.House_Sparrow',\n",
       " '119.Field_Sparrow',\n",
       " '120.Fox_Sparrow',\n",
       " '121.Grasshopper_Sparrow',\n",
       " '122.Harris_Sparrow',\n",
       " '123.Henslow_Sparrow',\n",
       " '124.Le_Conte_Sparrow',\n",
       " '125.Lincoln_Sparrow',\n",
       " '126.Nelson_Sharp_tailed_Sparrow',\n",
       " '127.Savannah_Sparrow',\n",
       " '128.Seaside_Sparrow',\n",
       " '129.Song_Sparrow',\n",
       " '130.Tree_Sparrow',\n",
       " '131.Vesper_Sparrow',\n",
       " '132.White_crowned_Sparrow',\n",
       " '133.White_throated_Sparrow',\n",
       " '134.Cape_Glossy_Starling',\n",
       " '135.Bank_Swallow',\n",
       " '136.Barn_Swallow',\n",
       " '137.Cliff_Swallow',\n",
       " '138.Tree_Swallow',\n",
       " '139.Scarlet_Tanager',\n",
       " '140.Summer_Tanager',\n",
       " '141.Artic_Tern',\n",
       " '142.Black_Tern',\n",
       " '143.Caspian_Tern',\n",
       " '144.Common_Tern',\n",
       " '145.Elegant_Tern',\n",
       " '146.Forsters_Tern',\n",
       " '147.Least_Tern',\n",
       " '148.Green_tailed_Towhee',\n",
       " '149.Brown_Thrasher',\n",
       " '150.Sage_Thrasher',\n",
       " '151.Black_capped_Vireo',\n",
       " '152.Blue_headed_Vireo',\n",
       " '153.Philadelphia_Vireo',\n",
       " '154.Red_eyed_Vireo',\n",
       " '155.Warbling_Vireo',\n",
       " '156.White_eyed_Vireo',\n",
       " '157.Yellow_throated_Vireo',\n",
       " '158.Bay_breasted_Warbler',\n",
       " '159.Black_and_white_Warbler',\n",
       " '160.Black_throated_Blue_Warbler',\n",
       " '161.Blue_winged_Warbler',\n",
       " '162.Canada_Warbler',\n",
       " '163.Cape_May_Warbler',\n",
       " '164.Cerulean_Warbler',\n",
       " '165.Chestnut_sided_Warbler',\n",
       " '166.Golden_winged_Warbler',\n",
       " '167.Hooded_Warbler',\n",
       " '168.Kentucky_Warbler',\n",
       " '169.Magnolia_Warbler',\n",
       " '170.Mourning_Warbler',\n",
       " '171.Myrtle_Warbler',\n",
       " '172.Nashville_Warbler',\n",
       " '173.Orange_crowned_Warbler',\n",
       " '174.Palm_Warbler',\n",
       " '175.Pine_Warbler',\n",
       " '176.Prairie_Warbler',\n",
       " '177.Prothonotary_Warbler',\n",
       " '178.Swainson_Warbler',\n",
       " '179.Tennessee_Warbler',\n",
       " '180.Wilson_Warbler',\n",
       " '181.Worm_eating_Warbler',\n",
       " '182.Yellow_Warbler',\n",
       " '183.Northern_Waterthrush',\n",
       " '184.Louisiana_Waterthrush',\n",
       " '185.Bohemian_Waxwing',\n",
       " '186.Cedar_Waxwing',\n",
       " '187.American_Three_toed_Woodpecker',\n",
       " '188.Pileated_Woodpecker',\n",
       " '189.Red_bellied_Woodpecker',\n",
       " '190.Red_cockaded_Woodpecker',\n",
       " '191.Red_headed_Woodpecker',\n",
       " '192.Downy_Woodpecker',\n",
       " '193.Bewick_Wren',\n",
       " '194.Cactus_Wren',\n",
       " '195.Carolina_Wren',\n",
       " '196.House_Wren',\n",
       " '197.Marsh_Wren',\n",
       " '198.Rock_Wren',\n",
       " '199.Winter_Wren',\n",
       " '200.Common_Yellowthroat']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aabfd469-eed2-49f6-be74-d14608f07c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class name: 001.Black_footed_Albatross, Integer label: 0\n",
      "Class name: 002.Laysan_Albatross, Integer label: 1\n",
      "Class name: 003.Sooty_Albatross, Integer label: 2\n",
      "Class name: 004.Groove_billed_Ani, Integer label: 3\n",
      "Class name: 005.Crested_Auklet, Integer label: 4\n",
      "Class name: 006.Least_Auklet, Integer label: 5\n",
      "Class name: 007.Parakeet_Auklet, Integer label: 6\n",
      "Class name: 008.Rhinoceros_Auklet, Integer label: 7\n",
      "Class name: 009.Brewer_Blackbird, Integer label: 8\n",
      "Class name: 010.Red_winged_Blackbird, Integer label: 9\n",
      "Class name: 011.Rusty_Blackbird, Integer label: 10\n",
      "Class name: 012.Yellow_headed_Blackbird, Integer label: 11\n",
      "Class name: 013.Bobolink, Integer label: 12\n",
      "Class name: 014.Indigo_Bunting, Integer label: 13\n",
      "Class name: 015.Lazuli_Bunting, Integer label: 14\n",
      "Class name: 016.Painted_Bunting, Integer label: 15\n",
      "Class name: 017.Cardinal, Integer label: 16\n",
      "Class name: 018.Spotted_Catbird, Integer label: 17\n",
      "Class name: 019.Gray_Catbird, Integer label: 18\n",
      "Class name: 020.Yellow_breasted_Chat, Integer label: 19\n",
      "Class name: 021.Eastern_Towhee, Integer label: 20\n",
      "Class name: 022.Chuck_will_Widow, Integer label: 21\n",
      "Class name: 023.Brandt_Cormorant, Integer label: 22\n",
      "Class name: 024.Red_faced_Cormorant, Integer label: 23\n",
      "Class name: 025.Pelagic_Cormorant, Integer label: 24\n",
      "Class name: 026.Bronzed_Cowbird, Integer label: 25\n",
      "Class name: 027.Shiny_Cowbird, Integer label: 26\n",
      "Class name: 028.Brown_Creeper, Integer label: 27\n",
      "Class name: 029.American_Crow, Integer label: 28\n",
      "Class name: 030.Fish_Crow, Integer label: 29\n",
      "Class name: 031.Black_billed_Cuckoo, Integer label: 30\n",
      "Class name: 032.Mangrove_Cuckoo, Integer label: 31\n",
      "Class name: 033.Yellow_billed_Cuckoo, Integer label: 32\n",
      "Class name: 034.Gray_crowned_Rosy_Finch, Integer label: 33\n",
      "Class name: 035.Purple_Finch, Integer label: 34\n",
      "Class name: 036.Northern_Flicker, Integer label: 35\n",
      "Class name: 037.Acadian_Flycatcher, Integer label: 36\n",
      "Class name: 038.Great_Crested_Flycatcher, Integer label: 37\n",
      "Class name: 039.Least_Flycatcher, Integer label: 38\n",
      "Class name: 040.Olive_sided_Flycatcher, Integer label: 39\n",
      "Class name: 041.Scissor_tailed_Flycatcher, Integer label: 40\n",
      "Class name: 042.Vermilion_Flycatcher, Integer label: 41\n",
      "Class name: 043.Yellow_bellied_Flycatcher, Integer label: 42\n",
      "Class name: 044.Frigatebird, Integer label: 43\n",
      "Class name: 045.Northern_Fulmar, Integer label: 44\n",
      "Class name: 046.Gadwall, Integer label: 45\n",
      "Class name: 047.American_Goldfinch, Integer label: 46\n",
      "Class name: 048.European_Goldfinch, Integer label: 47\n",
      "Class name: 049.Boat_tailed_Grackle, Integer label: 48\n",
      "Class name: 050.Eared_Grebe, Integer label: 49\n",
      "Class name: 051.Horned_Grebe, Integer label: 50\n",
      "Class name: 052.Pied_billed_Grebe, Integer label: 51\n",
      "Class name: 053.Western_Grebe, Integer label: 52\n",
      "Class name: 054.Blue_Grosbeak, Integer label: 53\n",
      "Class name: 055.Evening_Grosbeak, Integer label: 54\n",
      "Class name: 056.Pine_Grosbeak, Integer label: 55\n",
      "Class name: 057.Rose_breasted_Grosbeak, Integer label: 56\n",
      "Class name: 058.Pigeon_Guillemot, Integer label: 57\n",
      "Class name: 059.California_Gull, Integer label: 58\n",
      "Class name: 060.Glaucous_winged_Gull, Integer label: 59\n",
      "Class name: 061.Heermann_Gull, Integer label: 60\n",
      "Class name: 062.Herring_Gull, Integer label: 61\n",
      "Class name: 063.Ivory_Gull, Integer label: 62\n",
      "Class name: 064.Ring_billed_Gull, Integer label: 63\n",
      "Class name: 065.Slaty_backed_Gull, Integer label: 64\n",
      "Class name: 066.Western_Gull, Integer label: 65\n",
      "Class name: 067.Anna_Hummingbird, Integer label: 66\n",
      "Class name: 068.Ruby_throated_Hummingbird, Integer label: 67\n",
      "Class name: 069.Rufous_Hummingbird, Integer label: 68\n",
      "Class name: 070.Green_Violetear, Integer label: 69\n",
      "Class name: 071.Long_tailed_Jaeger, Integer label: 70\n",
      "Class name: 072.Pomarine_Jaeger, Integer label: 71\n",
      "Class name: 073.Blue_Jay, Integer label: 72\n",
      "Class name: 074.Florida_Jay, Integer label: 73\n",
      "Class name: 075.Green_Jay, Integer label: 74\n",
      "Class name: 076.Dark_eyed_Junco, Integer label: 75\n",
      "Class name: 077.Tropical_Kingbird, Integer label: 76\n",
      "Class name: 078.Gray_Kingbird, Integer label: 77\n",
      "Class name: 079.Belted_Kingfisher, Integer label: 78\n",
      "Class name: 080.Green_Kingfisher, Integer label: 79\n",
      "Class name: 081.Pied_Kingfisher, Integer label: 80\n",
      "Class name: 082.Ringed_Kingfisher, Integer label: 81\n",
      "Class name: 083.White_breasted_Kingfisher, Integer label: 82\n",
      "Class name: 084.Red_legged_Kittiwake, Integer label: 83\n",
      "Class name: 085.Horned_Lark, Integer label: 84\n",
      "Class name: 086.Pacific_Loon, Integer label: 85\n",
      "Class name: 087.Mallard, Integer label: 86\n",
      "Class name: 088.Western_Meadowlark, Integer label: 87\n",
      "Class name: 089.Hooded_Merganser, Integer label: 88\n",
      "Class name: 090.Red_breasted_Merganser, Integer label: 89\n",
      "Class name: 091.Mockingbird, Integer label: 90\n",
      "Class name: 092.Nighthawk, Integer label: 91\n",
      "Class name: 093.Clark_Nutcracker, Integer label: 92\n",
      "Class name: 094.White_breasted_Nuthatch, Integer label: 93\n",
      "Class name: 095.Baltimore_Oriole, Integer label: 94\n",
      "Class name: 096.Hooded_Oriole, Integer label: 95\n",
      "Class name: 097.Orchard_Oriole, Integer label: 96\n",
      "Class name: 098.Scott_Oriole, Integer label: 97\n",
      "Class name: 099.Ovenbird, Integer label: 98\n",
      "Class name: 100.Brown_Pelican, Integer label: 99\n",
      "Class name: 101.White_Pelican, Integer label: 100\n",
      "Class name: 102.Western_Wood_Pewee, Integer label: 101\n",
      "Class name: 103.Sayornis, Integer label: 102\n",
      "Class name: 104.American_Pipit, Integer label: 103\n",
      "Class name: 105.Whip_poor_Will, Integer label: 104\n",
      "Class name: 106.Horned_Puffin, Integer label: 105\n",
      "Class name: 107.Common_Raven, Integer label: 106\n",
      "Class name: 108.White_necked_Raven, Integer label: 107\n",
      "Class name: 109.American_Redstart, Integer label: 108\n",
      "Class name: 110.Geococcyx, Integer label: 109\n",
      "Class name: 111.Loggerhead_Shrike, Integer label: 110\n",
      "Class name: 112.Great_Grey_Shrike, Integer label: 111\n",
      "Class name: 113.Baird_Sparrow, Integer label: 112\n",
      "Class name: 114.Black_throated_Sparrow, Integer label: 113\n",
      "Class name: 115.Brewer_Sparrow, Integer label: 114\n",
      "Class name: 116.Chipping_Sparrow, Integer label: 115\n",
      "Class name: 117.Clay_colored_Sparrow, Integer label: 116\n",
      "Class name: 118.House_Sparrow, Integer label: 117\n",
      "Class name: 119.Field_Sparrow, Integer label: 118\n",
      "Class name: 120.Fox_Sparrow, Integer label: 119\n",
      "Class name: 121.Grasshopper_Sparrow, Integer label: 120\n",
      "Class name: 122.Harris_Sparrow, Integer label: 121\n",
      "Class name: 123.Henslow_Sparrow, Integer label: 122\n",
      "Class name: 124.Le_Conte_Sparrow, Integer label: 123\n",
      "Class name: 125.Lincoln_Sparrow, Integer label: 124\n",
      "Class name: 126.Nelson_Sharp_tailed_Sparrow, Integer label: 125\n",
      "Class name: 127.Savannah_Sparrow, Integer label: 126\n",
      "Class name: 128.Seaside_Sparrow, Integer label: 127\n",
      "Class name: 129.Song_Sparrow, Integer label: 128\n",
      "Class name: 130.Tree_Sparrow, Integer label: 129\n",
      "Class name: 131.Vesper_Sparrow, Integer label: 130\n",
      "Class name: 132.White_crowned_Sparrow, Integer label: 131\n",
      "Class name: 133.White_throated_Sparrow, Integer label: 132\n",
      "Class name: 134.Cape_Glossy_Starling, Integer label: 133\n",
      "Class name: 135.Bank_Swallow, Integer label: 134\n",
      "Class name: 136.Barn_Swallow, Integer label: 135\n",
      "Class name: 137.Cliff_Swallow, Integer label: 136\n",
      "Class name: 138.Tree_Swallow, Integer label: 137\n",
      "Class name: 139.Scarlet_Tanager, Integer label: 138\n",
      "Class name: 140.Summer_Tanager, Integer label: 139\n",
      "Class name: 141.Artic_Tern, Integer label: 140\n",
      "Class name: 142.Black_Tern, Integer label: 141\n",
      "Class name: 143.Caspian_Tern, Integer label: 142\n",
      "Class name: 144.Common_Tern, Integer label: 143\n",
      "Class name: 145.Elegant_Tern, Integer label: 144\n",
      "Class name: 146.Forsters_Tern, Integer label: 145\n",
      "Class name: 147.Least_Tern, Integer label: 146\n",
      "Class name: 148.Green_tailed_Towhee, Integer label: 147\n",
      "Class name: 149.Brown_Thrasher, Integer label: 148\n",
      "Class name: 150.Sage_Thrasher, Integer label: 149\n",
      "Class name: 151.Black_capped_Vireo, Integer label: 150\n",
      "Class name: 152.Blue_headed_Vireo, Integer label: 151\n",
      "Class name: 153.Philadelphia_Vireo, Integer label: 152\n",
      "Class name: 154.Red_eyed_Vireo, Integer label: 153\n",
      "Class name: 155.Warbling_Vireo, Integer label: 154\n",
      "Class name: 156.White_eyed_Vireo, Integer label: 155\n",
      "Class name: 157.Yellow_throated_Vireo, Integer label: 156\n",
      "Class name: 158.Bay_breasted_Warbler, Integer label: 157\n",
      "Class name: 159.Black_and_white_Warbler, Integer label: 158\n",
      "Class name: 160.Black_throated_Blue_Warbler, Integer label: 159\n",
      "Class name: 161.Blue_winged_Warbler, Integer label: 160\n",
      "Class name: 162.Canada_Warbler, Integer label: 161\n",
      "Class name: 163.Cape_May_Warbler, Integer label: 162\n",
      "Class name: 164.Cerulean_Warbler, Integer label: 163\n",
      "Class name: 165.Chestnut_sided_Warbler, Integer label: 164\n",
      "Class name: 166.Golden_winged_Warbler, Integer label: 165\n",
      "Class name: 167.Hooded_Warbler, Integer label: 166\n",
      "Class name: 168.Kentucky_Warbler, Integer label: 167\n",
      "Class name: 169.Magnolia_Warbler, Integer label: 168\n",
      "Class name: 170.Mourning_Warbler, Integer label: 169\n",
      "Class name: 171.Myrtle_Warbler, Integer label: 170\n",
      "Class name: 172.Nashville_Warbler, Integer label: 171\n",
      "Class name: 173.Orange_crowned_Warbler, Integer label: 172\n",
      "Class name: 174.Palm_Warbler, Integer label: 173\n",
      "Class name: 175.Pine_Warbler, Integer label: 174\n",
      "Class name: 176.Prairie_Warbler, Integer label: 175\n",
      "Class name: 177.Prothonotary_Warbler, Integer label: 176\n",
      "Class name: 178.Swainson_Warbler, Integer label: 177\n",
      "Class name: 179.Tennessee_Warbler, Integer label: 178\n",
      "Class name: 180.Wilson_Warbler, Integer label: 179\n",
      "Class name: 181.Worm_eating_Warbler, Integer label: 180\n",
      "Class name: 182.Yellow_Warbler, Integer label: 181\n",
      "Class name: 183.Northern_Waterthrush, Integer label: 182\n",
      "Class name: 184.Louisiana_Waterthrush, Integer label: 183\n",
      "Class name: 185.Bohemian_Waxwing, Integer label: 184\n",
      "Class name: 186.Cedar_Waxwing, Integer label: 185\n",
      "Class name: 187.American_Three_toed_Woodpecker, Integer label: 186\n",
      "Class name: 188.Pileated_Woodpecker, Integer label: 187\n",
      "Class name: 189.Red_bellied_Woodpecker, Integer label: 188\n",
      "Class name: 190.Red_cockaded_Woodpecker, Integer label: 189\n",
      "Class name: 191.Red_headed_Woodpecker, Integer label: 190\n",
      "Class name: 192.Downy_Woodpecker, Integer label: 191\n",
      "Class name: 193.Bewick_Wren, Integer label: 192\n",
      "Class name: 194.Cactus_Wren, Integer label: 193\n",
      "Class name: 195.Carolina_Wren, Integer label: 194\n",
      "Class name: 196.House_Wren, Integer label: 195\n",
      "Class name: 197.Marsh_Wren, Integer label: 196\n",
      "Class name: 198.Rock_Wren, Integer label: 197\n",
      "Class name: 199.Winter_Wren, Integer label: 198\n",
      "Class name: 200.Common_Yellowthroat, Integer label: 199\n"
     ]
    }
   ],
   "source": [
    "for i, name in enumerate(class_names):\n",
    "    label_int = label_lookup(tf.constant(name)).numpy()\n",
    "    print(f\"Class name: {name}, Integer label: {label_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb9d69-40a9-4bbc-84d3-3acbab6be094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "795c2954-4c10-48f9-a6b1-2ee27e41727b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(augmented_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19e74bf9-1eaa-4ae1-9f4b-66279229f4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(295, 36, 38)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test), len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c893fcd-dfb2-4479-a179-149ecade682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to the range [0, 1]\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Apply normalization to the dataset\n",
    "normalized_train = augmented_train.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ee77d53-baff-4214-95a3-2f7d44612ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_test = test.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18987ec4-fb7f-4a1b-ad15-cd4c865e0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_validation = validation.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1385f970-5da3-4af4-8398-c9a35c3ae5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape=(224, 224, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1), padding='same'),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5), strides=(2, 2), padding='same'),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Third Convolutional Block\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1)),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),  # Pooling (2,2)\n",
    "    \n",
    "    # Fourth Convolutional Block\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same'),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),  # Pooling (2,2)\n",
    "    \n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    # Fifth Convolutional Block\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same'),  # Corrected typo here\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),  # Pooling (2,2)\n",
    "    \n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='softmax')  # Output layer with 525 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec95362c-6439-4723-95c2-12d8d3cdfc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">25,632</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">25,800</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │              \u001b[38;5;34m12\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m2,432\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │          \u001b[38;5;34m25,632\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m147,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │          \u001b[38;5;34m25,800\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">332,020</span> (1.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m332,020\u001b[0m (1.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">331,374</span> (1.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m331,374\u001b[0m (1.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">646</span> (2.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m646\u001b[0m (2.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2928498c-6bd4-4acc-992b-5946ab022096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "087d456f-b3d9-411a-9465-15096cfd17bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m701s\u001b[0m 2s/step - accuracy: 0.0057 - loss: 5.4995 - val_accuracy: 0.0000e+00 - val_loss: 5.8114\n",
      "Epoch 2/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m539s\u001b[0m 2s/step - accuracy: 0.0100 - loss: 5.2378 - val_accuracy: 0.0000e+00 - val_loss: 5.2766\n",
      "Epoch 3/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m749s\u001b[0m 2s/step - accuracy: 0.0118 - loss: 5.1324 - val_accuracy: 0.0000e+00 - val_loss: 5.1018\n",
      "Epoch 4/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 2s/step - accuracy: 0.0199 - loss: 4.9992 - val_accuracy: 0.0625 - val_loss: 5.0855\n",
      "Epoch 5/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5433s\u001b[0m 18s/step - accuracy: 0.0238 - loss: 4.8014 - val_accuracy: 0.0312 - val_loss: 4.4897\n",
      "Epoch 6/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1219s\u001b[0m 4s/step - accuracy: 0.0331 - loss: 4.6032 - val_accuracy: 0.0000e+00 - val_loss: 4.4631\n",
      "Epoch 7/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1994s\u001b[0m 6s/step - accuracy: 0.0531 - loss: 4.3919 - val_accuracy: 0.0938 - val_loss: 3.9790\n",
      "Epoch 8/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m913s\u001b[0m 3s/step - accuracy: 0.0548 - loss: 4.2477 - val_accuracy: 0.1250 - val_loss: 3.8728\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "\n",
    "# Train with callbacks\n",
    "history = model.fit(\n",
    "    normalized_train,\n",
    "    epochs=10,\n",
    "    validation_data=normalized_validation,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3e8df5e-a42f-4f55-b861-6fd657d5f480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape=(224, 224, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5), strides=(2, 2), padding='same'),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same'),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "  \n",
    "    \n",
    "    # Third Convolutional Block\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    # fourth Convolutional Block\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Fifth Convolutional Block\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same',kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    # Sixth Convolutional Block\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same',kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Corrected typo here\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    #seventh Convolutional Block\n",
    "     tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same',kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Corrected typo here\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "   \n",
    "    #eight Convolutional Block\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same',kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Corrected typo here\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    # Ninth Convolutional Block\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same',kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Corrected typo here\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),  # Pooling (2,2)\n",
    "    \n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='softmax')  # Output layer with 200 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78e110a5-049a-4d8c-938e-82093b4a1585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21632</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,769,024</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">25,800</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │              \u001b[38;5;34m12\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m2,432\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m9,248\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m9,248\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m9,248\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_6 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_7 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ leaky_re_lu_8 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21632\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m2,769,024\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │          \u001b[38;5;34m25,800\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,030,196</span> (11.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,030,196\u001b[0m (11.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,029,166</span> (11.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,029,166\u001b[0m (11.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,030</span> (4.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,030\u001b[0m (4.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95fbfea3-d344-4f80-ab2b-1e8213ba6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d93694-78c4-4a3a-adc4-a6c69c1fe926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1611s\u001b[0m 5s/step - accuracy: 0.0048 - loss: 6.0466 - val_accuracy: 0.0041 - val_loss: 14.2746 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1644s\u001b[0m 5s/step - accuracy: 0.0051 - loss: 5.6258 - val_accuracy: 0.0017 - val_loss: 12.6070 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1689s\u001b[0m 6s/step - accuracy: 0.0043 - loss: 5.5499 - val_accuracy: 0.0066 - val_loss: 18.1829 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1941s\u001b[0m 6s/step - accuracy: 0.0051 - loss: 5.5784 - val_accuracy: 0.0074 - val_loss: 5.5973 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m 64/295\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:11:37\u001b[0m 34s/step - accuracy: 0.0055 - loss: 5.5693"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m lr_schedule \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train with callbacks\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalized_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalized_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "# Train with callbacks\n",
    "history = model2.fit(\n",
    "    normalized_train,\n",
    "    epochs=30,\n",
    "    validation_data=normalized_validation,\n",
    "    callbacks=[early_stopping, model_checkpoint,lr_schedule]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5151b205-9481-441a-a25c-7331ef012239",
   "metadata": {},
   "source": [
    "Using Transfer Learning which utilizes pre-trained models that have been trained on bigger datasets which helps them to learn features and this features can be utilized for the task of bird species classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9c3e743-3023-45e3-9de3-126a853eeec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">51,400</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224 (\u001b[38;5;33mFunctional\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m327,936\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │          \u001b[38;5;34m51,400\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,637,320</span> (10.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,637,320\u001b[0m (10.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">379,336</span> (1.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m379,336\u001b[0m (1.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load pre-trained MobileNetV2 without the top classification layer\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3),  # Your image size\n",
    "    include_top=False,  # Remove the final dense layers (ImageNet's output)\n",
    "    weights='imagenet'  # Use pre-trained ImageNet weights\n",
    ")\n",
    "\n",
    "# Freeze the base model layers (so they are not updated during training)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create your model and add custom layers on top of the base model\n",
    "trmodel = models.Sequential([\n",
    "    base_model,  # Pre-trained MobileNetV2 layers\n",
    "    layers.GlobalAveragePooling2D(),  # Reduce feature map to a vector by averaging\n",
    "    layers.Dense(256, activation='relu'),  # Add a dense layer for learning\n",
    "    layers.Dropout(0.5),  # Dropout for regularization\n",
    "    layers.Dense(200, activation='softmax')  # Final layer for your 200 bird species\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "trmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "trmodel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "824e7429-85f2-4677-b138-c35c72e049f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 1s/step - accuracy: 0.0126 - loss: 5.2774 - val_accuracy: 0.1069 - val_loss: 4.2033 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 1s/step - accuracy: 0.0599 - loss: 4.3711 - val_accuracy: 0.2333 - val_loss: 3.4338 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 1s/step - accuracy: 0.1138 - loss: 3.8716 - val_accuracy: 0.2870 - val_loss: 3.0218 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 1s/step - accuracy: 0.1321 - loss: 3.6410 - val_accuracy: 0.3520 - val_loss: 2.7073 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 1s/step - accuracy: 0.1589 - loss: 3.4847 - val_accuracy: 0.4005 - val_loss: 2.5358 - learning_rate: 0.0010\n",
      "Epoch 6/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 1s/step - accuracy: 0.1796 - loss: 3.3491 - val_accuracy: 0.3602 - val_loss: 2.4499 - learning_rate: 0.0010\n",
      "Epoch 7/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 1s/step - accuracy: 0.2023 - loss: 3.2541 - val_accuracy: 0.3946 - val_loss: 2.4200 - learning_rate: 0.0010\n",
      "Epoch 8/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 1s/step - accuracy: 0.2153 - loss: 3.1336 - val_accuracy: 0.4005 - val_loss: 2.3278 - learning_rate: 0.0010\n",
      "Epoch 9/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 1s/step - accuracy: 0.2088 - loss: 3.1533 - val_accuracy: 0.4356 - val_loss: 2.3014 - learning_rate: 0.0010\n",
      "Epoch 10/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 1s/step - accuracy: 0.2254 - loss: 3.0497 - val_accuracy: 0.4214 - val_loss: 2.2760 - learning_rate: 0.0010\n",
      "Epoch 11/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 1s/step - accuracy: 0.2385 - loss: 3.0259 - val_accuracy: 0.4227 - val_loss: 2.2280 - learning_rate: 0.0010\n",
      "Epoch 12/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 1s/step - accuracy: 0.2403 - loss: 2.9980 - val_accuracy: 0.4696 - val_loss: 2.0825 - learning_rate: 0.0010\n",
      "Epoch 13/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 1s/step - accuracy: 0.2395 - loss: 3.0389 - val_accuracy: 0.4556 - val_loss: 2.1620 - learning_rate: 0.0010\n",
      "Epoch 14/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 2s/step - accuracy: 0.2474 - loss: 2.9514 - val_accuracy: 0.4474 - val_loss: 2.1194 - learning_rate: 0.0010\n",
      "Epoch 15/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m660s\u001b[0m 2s/step - accuracy: 0.2529 - loss: 2.9616 - val_accuracy: 0.4581 - val_loss: 2.0607 - learning_rate: 0.0010\n",
      "Epoch 16/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m703s\u001b[0m 2s/step - accuracy: 0.2606 - loss: 2.8804 - val_accuracy: 0.4556 - val_loss: 2.0175 - learning_rate: 0.0010\n",
      "Epoch 17/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 2s/step - accuracy: 0.2446 - loss: 2.9412 - val_accuracy: 0.4507 - val_loss: 2.0691 - learning_rate: 0.0010\n",
      "Epoch 18/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 2s/step - accuracy: 0.2573 - loss: 2.8951 - val_accuracy: 0.4770 - val_loss: 2.0429 - learning_rate: 0.0010\n",
      "Epoch 19/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m637s\u001b[0m 2s/step - accuracy: 0.2652 - loss: 2.8738 - val_accuracy: 0.4827 - val_loss: 1.9572 - learning_rate: 0.0010\n",
      "Epoch 20/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 2s/step - accuracy: 0.2546 - loss: 2.8981 - val_accuracy: 0.4679 - val_loss: 1.9863 - learning_rate: 0.0010\n",
      "Epoch 21/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m660s\u001b[0m 2s/step - accuracy: 0.2807 - loss: 2.8182 - val_accuracy: 0.4655 - val_loss: 1.9692 - learning_rate: 0.0010\n",
      "Epoch 22/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2722 - loss: 2.8282\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 2s/step - accuracy: 0.2722 - loss: 2.8282 - val_accuracy: 0.4655 - val_loss: 1.9820 - learning_rate: 0.0010\n",
      "Epoch 23/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m648s\u001b[0m 2s/step - accuracy: 0.2896 - loss: 2.7482 - val_accuracy: 0.5025 - val_loss: 1.8378 - learning_rate: 5.0000e-04\n",
      "Epoch 24/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 2s/step - accuracy: 0.2881 - loss: 2.7508 - val_accuracy: 0.4908 - val_loss: 1.8874 - learning_rate: 5.0000e-04\n",
      "Epoch 25/25\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 2s/step - accuracy: 0.3030 - loss: 2.6866 - val_accuracy: 0.5109 - val_loss: 1.8046 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_lr_model.keras', save_best_only=True)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "# Train with callbacks\n",
    "history = trmodel.fit(\n",
    "    normalized_train,\n",
    "    epochs=25,\n",
    "    validation_data=normalized_validation,\n",
    "    callbacks=[early_stopping, model_checkpoint,lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fdf9c62-7c09-472c-947a-c17b05e542fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the latest checkpoint if saved\n",
    "trmodelcontinue = load_model('best_transfer_lr_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a6c6c6a-a794-4a52-9d49-38d635ab5636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/35\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 1s/step - accuracy: 0.3065 - loss: 2.6671 - val_accuracy: 0.5321 - val_loss: 1.7851 - learning_rate: 5.0000e-04\n",
      "Epoch 27/35\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 1s/step - accuracy: 0.3135 - loss: 2.6378 - val_accuracy: 0.5008 - val_loss: 1.8176 - learning_rate: 5.0000e-04\n",
      "Epoch 28/35\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 1s/step - accuracy: 0.3068 - loss: 2.6320 - val_accuracy: 0.5132 - val_loss: 1.8575 - learning_rate: 5.0000e-04\n",
      "Epoch 29/35\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 1s/step - accuracy: 0.3174 - loss: 2.6405 - val_accuracy: 0.5214 - val_loss: 1.7695 - learning_rate: 5.0000e-04\n",
      "Epoch 30/35\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 1s/step - accuracy: 0.3034 - loss: 2.6912 - val_accuracy: 0.5296 - val_loss: 1.7361 - learning_rate: 5.0000e-04\n",
      "Epoch 31/35\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.3106 - loss: 2.6220 - val_accuracy: 0.4984 - val_loss: 1.8510 - learning_rate: 5.0000e-04\n",
      "Epoch 32/35\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 1s/step - accuracy: 0.3232 - loss: 2.6095 - val_accuracy: 0.5125 - val_loss: 1.8462 - learning_rate: 5.0000e-04\n",
      "Epoch 33/35\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.3081 - loss: 2.6536 - val_accuracy: 0.5132 - val_loss: 1.8387 - learning_rate: 5.0000e-04\n",
      "Epoch 34/35\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923ms/step - accuracy: 0.3217 - loss: 2.6079\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 1s/step - accuracy: 0.3217 - loss: 2.6079 - val_accuracy: 0.5452 - val_loss: 1.7699 - learning_rate: 5.0000e-04\n",
      "Epoch 35/35\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 1s/step - accuracy: 0.3288 - loss: 2.5776 - val_accuracy: 0.5293 - val_loss: 1.8010 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_lr_model.keras', save_best_only=True)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "history = trmodelcontinue.fit(\n",
    "    normalized_train,\n",
    "    epochs=35,  # Total epochs (25 already done + 10 more)\n",
    "    initial_epoch=25,  # Start from epoch 26\n",
    "    validation_data=normalized_validation,\n",
    "    callbacks=[early_stopping, model_checkpoint, lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50d0b0c0-34d6-4d0a-8291-dff482ab5953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/45\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2448 - loss: 3.0638\n",
      "Epoch 36: val_loss improved from inf to 1.85154, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 2s/step - accuracy: 0.2448 - loss: 3.0635 - val_accuracy: 0.4707 - val_loss: 1.8515 - learning_rate: 1.0000e-04\n",
      "Epoch 37/45\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2846 - loss: 2.8270\n",
      "Epoch 37: val_loss improved from 1.85154 to 1.73830, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m729s\u001b[0m 2s/step - accuracy: 0.2846 - loss: 2.8271 - val_accuracy: 0.5230 - val_loss: 1.7383 - learning_rate: 1.0000e-04\n",
      "Epoch 38/45\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2917 - loss: 2.7622\n",
      "Epoch 38: val_loss did not improve from 1.73830\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m704s\u001b[0m 2s/step - accuracy: 0.2917 - loss: 2.7622 - val_accuracy: 0.4844 - val_loss: 1.8004 - learning_rate: 1.0000e-04\n",
      "Epoch 39/45\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2975 - loss: 2.6827\n",
      "Epoch 39: val_loss improved from 1.73830 to 1.70977, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m660s\u001b[0m 2s/step - accuracy: 0.2975 - loss: 2.6828 - val_accuracy: 0.5214 - val_loss: 1.7098 - learning_rate: 1.0000e-04\n",
      "Epoch 40/45\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3041 - loss: 2.6872\n",
      "Epoch 40: val_loss did not improve from 1.70977\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m670s\u001b[0m 2s/step - accuracy: 0.3041 - loss: 2.6872 - val_accuracy: 0.5041 - val_loss: 1.7996 - learning_rate: 1.0000e-04\n",
      "Epoch 41/45\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3115 - loss: 2.5953\n",
      "Epoch 41: val_loss did not improve from 1.70977\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m660s\u001b[0m 2s/step - accuracy: 0.3115 - loss: 2.5954 - val_accuracy: 0.5059 - val_loss: 1.8233 - learning_rate: 1.0000e-04\n",
      "Epoch 42/45\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3252 - loss: 2.5864\n",
      "Epoch 42: val_loss did not improve from 1.70977\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m705s\u001b[0m 2s/step - accuracy: 0.3252 - loss: 2.5865 - val_accuracy: 0.5074 - val_loss: 1.8111 - learning_rate: 1.0000e-04\n",
      "Epoch 43/45\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3205 - loss: 2.6314\n",
      "Epoch 43: val_loss did not improve from 1.70977\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m769s\u001b[0m 2s/step - accuracy: 0.3205 - loss: 2.6312 - val_accuracy: 0.5296 - val_loss: 1.7348 - learning_rate: 5.0000e-05\n",
      "Epoch 44/45\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3347 - loss: 2.5723\n",
      "Epoch 44: val_loss improved from 1.70977 to 1.63227, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m662s\u001b[0m 2s/step - accuracy: 0.3347 - loss: 2.5723 - val_accuracy: 0.5789 - val_loss: 1.6323 - learning_rate: 5.0000e-05\n",
      "Epoch 45/45\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3420 - loss: 2.4883\n",
      "Epoch 45: val_loss improved from 1.63227 to 1.61656, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 2s/step - accuracy: 0.3419 - loss: 2.4885 - val_accuracy: 0.5559 - val_loss: 1.6166 - learning_rate: 5.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 45.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the previously saved model\n",
    "trmodel = load_model('best_transfer_lr_model.keras')\n",
    "\n",
    "# Unfreeze only the last few layers of MobileNet for fine-tuning\n",
    "# Assuming 'mobilenet' is part of the model\n",
    "mobilenet = trmodel.get_layer('mobilenetv2_1.00_224')  # Change this if needed\n",
    "\n",
    "# Unfreeze the last few layers\n",
    "for layer in mobilenet.layers[-10:]:  # Adjust number of layers to unfreeze\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model again after unfreezing layers\n",
    "trmodel.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),  # You may want to lower the learning rate for fine-tuning\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "# Continue training the model\n",
    "history = trmodel.fit(\n",
    "    normalized_train,  # Your training dataset\n",
    "    epochs=45,  # Continue training for 10 more epochs\n",
    "    initial_epoch=35,\n",
    "    validation_data=normalized_validation,  # Your validation dataset\n",
    "    callbacks=[early_stopping, model_checkpoint, lr_schedule]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "887b4e2b-3131-4c77-b6b1-12198819be3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3379 - loss: 2.5077\n",
      "Epoch 1: val_loss improved from inf to 1.52895, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m795s\u001b[0m 2s/step - accuracy: 0.3379 - loss: 2.5076 - val_accuracy: 0.5543 - val_loss: 1.5289 - learning_rate: 5.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3600 - loss: 2.4233\n",
      "Epoch 2: val_loss improved from 1.52895 to 1.50805, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 2s/step - accuracy: 0.3599 - loss: 2.4234 - val_accuracy: 0.5781 - val_loss: 1.5081 - learning_rate: 5.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3488 - loss: 2.4391\n",
      "Epoch 3: val_loss did not improve from 1.50805\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 2s/step - accuracy: 0.3488 - loss: 2.4391 - val_accuracy: 0.5683 - val_loss: 1.5451 - learning_rate: 5.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3609 - loss: 2.4133\n",
      "Epoch 4: val_loss did not improve from 1.50805\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m667s\u001b[0m 2s/step - accuracy: 0.3609 - loss: 2.4134 - val_accuracy: 0.5444 - val_loss: 1.6506 - learning_rate: 5.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3617 - loss: 2.4233\n",
      "Epoch 5: val_loss improved from 1.50805 to 1.44844, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m700s\u001b[0m 2s/step - accuracy: 0.3617 - loss: 2.4232 - val_accuracy: 0.6127 - val_loss: 1.4484 - learning_rate: 5.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3564 - loss: 2.4190\n",
      "Epoch 6: val_loss did not improve from 1.44844\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m744s\u001b[0m 2s/step - accuracy: 0.3564 - loss: 2.4190 - val_accuracy: 0.5699 - val_loss: 1.5294 - learning_rate: 5.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3774 - loss: 2.3539\n",
      "Epoch 7: val_loss did not improve from 1.44844\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m704s\u001b[0m 2s/step - accuracy: 0.3774 - loss: 2.3540 - val_accuracy: 0.5699 - val_loss: 1.5422 - learning_rate: 5.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3696 - loss: 2.3992\n",
      "Epoch 8: val_loss did not improve from 1.44844\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m746s\u001b[0m 2s/step - accuracy: 0.3696 - loss: 2.3992 - val_accuracy: 0.5617 - val_loss: 1.5401 - learning_rate: 5.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3554 - loss: 2.3956\n",
      "Epoch 9: val_loss did not improve from 1.44844\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m748s\u001b[0m 2s/step - accuracy: 0.3554 - loss: 2.3955 - val_accuracy: 0.5896 - val_loss: 1.5112 - learning_rate: 5.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3621 - loss: 2.4080\n",
      "Epoch 10: val_loss did not improve from 1.44844\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27936s\u001b[0m 2s/step - accuracy: 0.3621 - loss: 2.4077 - val_accuracy: 0.5719 - val_loss: 1.5090 - learning_rate: 2.5000e-05\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the best saved model\n",
    "model = load_model('best_transfer_finetune_model.keras')\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=5e-5),  # Continue with the reduced learning rate\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "# Continue training the model for more epochs\n",
    "history = model.fit(\n",
    "    normalized_train,\n",
    "    # Your training dataset\n",
    "    epochs=10,\n",
    "    # Add 10 more epochs, adjust as needed\n",
    "    validation_data=normalized_validation,  # Your validation dataset\n",
    "    callbacks=[early_stopping, model_checkpoint, lr_schedule]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73441d9b-89fb-450a-b92d-7afa76e907fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the latest checkpoint if saved\n",
    "trmodelcontinue = load_model('best_transfer_finetune_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "583e7d70-c96f-4f88-b16b-c145f79c08d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3594 - loss: 2.4012\n",
      "Epoch 1: val_loss improved from inf to 1.49719, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m675s\u001b[0m 2s/step - accuracy: 0.3594 - loss: 2.4012 - val_accuracy: 0.5781 - val_loss: 1.4972 - learning_rate: 5.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3620 - loss: 2.3944\n",
      "Epoch 2: val_loss improved from 1.49719 to 1.46497, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 2s/step - accuracy: 0.3620 - loss: 2.3944 - val_accuracy: 0.5778 - val_loss: 1.4650 - learning_rate: 5.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3656 - loss: 2.3775\n",
      "Epoch 3: val_loss did not improve from 1.46497\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m714s\u001b[0m 2s/step - accuracy: 0.3656 - loss: 2.3775 - val_accuracy: 0.5617 - val_loss: 1.5406 - learning_rate: 5.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3695 - loss: 2.3793\n",
      "Epoch 4: val_loss did not improve from 1.46497\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m666s\u001b[0m 2s/step - accuracy: 0.3695 - loss: 2.3793 - val_accuracy: 0.5765 - val_loss: 1.5254 - learning_rate: 5.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3683 - loss: 2.3543\n",
      "Epoch 5: val_loss improved from 1.46497 to 1.41850, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m670s\u001b[0m 2s/step - accuracy: 0.3682 - loss: 2.3544 - val_accuracy: 0.6299 - val_loss: 1.4185 - learning_rate: 5.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3681 - loss: 2.3468\n",
      "Epoch 6: val_loss improved from 1.41850 to 1.40107, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m688s\u001b[0m 2s/step - accuracy: 0.3681 - loss: 2.3468 - val_accuracy: 0.6077 - val_loss: 1.4011 - learning_rate: 5.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3799 - loss: 2.2983\n",
      "Epoch 7: val_loss did not improve from 1.40107\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m752s\u001b[0m 2s/step - accuracy: 0.3799 - loss: 2.2983 - val_accuracy: 0.5970 - val_loss: 1.4275 - learning_rate: 5.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3666 - loss: 2.3225\n",
      "Epoch 8: val_loss did not improve from 1.40107\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 1s/step - accuracy: 0.3666 - loss: 2.3225 - val_accuracy: 0.5979 - val_loss: 1.5095 - learning_rate: 5.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965ms/step - accuracy: 0.3857 - loss: 2.2922\n",
      "Epoch 9: val_loss did not improve from 1.40107\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 1s/step - accuracy: 0.3857 - loss: 2.2923 - val_accuracy: 0.5853 - val_loss: 1.4797 - learning_rate: 5.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970ms/step - accuracy: 0.3903 - loss: 2.2740\n",
      "Epoch 10: val_loss did not improve from 1.40107\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 1s/step - accuracy: 0.3902 - loss: 2.2740 - val_accuracy: 0.5744 - val_loss: 1.4833 - learning_rate: 5.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "history = trmodelcontinue.fit(\n",
    "    normalized_train,\n",
    "    epochs=10,  # Total 10 more)\n",
    "    validation_data=normalized_validation,\n",
    "    callbacks=[early_stopping, model_checkpoint, lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5b542bc-07d2-4565-ade7-ad9dcc28688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the latest checkpoint if saved\n",
    "trmodelcontinue = load_model('best_transfer_finetune_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb1b4a23-b32e-490f-9a0d-0b31c73b5b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3757 - loss: 2.3526\n",
      "Epoch 1: val_loss improved from inf to 1.42424, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 1s/step - accuracy: 0.3757 - loss: 2.3525 - val_accuracy: 0.6151 - val_loss: 1.4242 - learning_rate: 5.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3748 - loss: 2.3509\n",
      "Epoch 2: val_loss did not improve from 1.42424\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 1s/step - accuracy: 0.3748 - loss: 2.3508 - val_accuracy: 0.6179 - val_loss: 1.4433 - learning_rate: 5.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993ms/step - accuracy: 0.3781 - loss: 2.3038\n",
      "Epoch 3: val_loss improved from 1.42424 to 1.34075, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 1s/step - accuracy: 0.3780 - loss: 2.3038 - val_accuracy: 0.6472 - val_loss: 1.3407 - learning_rate: 5.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3896 - loss: 2.3015\n",
      "Epoch 4: val_loss did not improve from 1.34075\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 1s/step - accuracy: 0.3896 - loss: 2.3015 - val_accuracy: 0.6061 - val_loss: 1.4614 - learning_rate: 5.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3871 - loss: 2.3019\n",
      "Epoch 5: val_loss did not improve from 1.34075\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 1s/step - accuracy: 0.3870 - loss: 2.3020 - val_accuracy: 0.5938 - val_loss: 1.3942 - learning_rate: 5.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3820 - loss: 2.2697\n",
      "Epoch 6: val_loss did not improve from 1.34075\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 1s/step - accuracy: 0.3820 - loss: 2.2696 - val_accuracy: 0.6192 - val_loss: 1.3839 - learning_rate: 5.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3878 - loss: 2.2694\n",
      "Epoch 7: val_loss did not improve from 1.34075\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 1s/step - accuracy: 0.3878 - loss: 2.2694 - val_accuracy: 0.6229 - val_loss: 1.3724 - learning_rate: 5.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4000 - loss: 2.2322\n",
      "Epoch 8: val_loss did not improve from 1.34075\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 1s/step - accuracy: 0.4000 - loss: 2.2322 - val_accuracy: 0.5970 - val_loss: 1.4103 - learning_rate: 2.5000e-05\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "history = trmodelcontinue.fit(\n",
    "    normalized_train,\n",
    "    epochs=10,  # Total 10 more)\n",
    "    validation_data=normalized_validation,\n",
    "    callbacks=[early_stopping, model_checkpoint, lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e25666-605d-4b7f-b679-1775416595ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/18\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3849 - loss: 2.2853\n",
      "Epoch 9: val_loss improved from inf to 1.42494, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m752s\u001b[0m 2s/step - accuracy: 0.3848 - loss: 2.2853 - val_accuracy: 0.5945 - val_loss: 1.4249 - learning_rate: 1.0000e-06\n",
      "Epoch 10/18\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3798 - loss: 2.3143\n",
      "Epoch 10: val_loss did not improve from 1.42494\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m758s\u001b[0m 2s/step - accuracy: 0.3798 - loss: 2.3143 - val_accuracy: 0.6003 - val_loss: 1.4466 - learning_rate: 1.0000e-06\n",
      "Epoch 11/18\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3868 - loss: 2.2824\n",
      "Epoch 11: val_loss improved from 1.42494 to 1.36917, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m798s\u001b[0m 3s/step - accuracy: 0.3868 - loss: 2.2824 - val_accuracy: 0.6102 - val_loss: 1.3692 - learning_rate: 1.0000e-06\n",
      "Epoch 12/18\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3850 - loss: 2.2870\n",
      "Epoch 12: val_loss did not improve from 1.36917\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m751s\u001b[0m 2s/step - accuracy: 0.3850 - loss: 2.2869 - val_accuracy: 0.6209 - val_loss: 1.3925 - learning_rate: 1.0000e-06\n",
      "Epoch 13/18\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3882 - loss: 2.3051\n",
      "Epoch 13: val_loss did not improve from 1.36917\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m748s\u001b[0m 2s/step - accuracy: 0.3882 - loss: 2.3050 - val_accuracy: 0.6250 - val_loss: 1.3745 - learning_rate: 1.0000e-06\n",
      "Epoch 14/18\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3930 - loss: 2.2507\n",
      "Epoch 14: val_loss improved from 1.36917 to 1.36131, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m773s\u001b[0m 2s/step - accuracy: 0.3930 - loss: 2.2506 - val_accuracy: 0.6201 - val_loss: 1.3613 - learning_rate: 1.0000e-06\n",
      "Epoch 15/18\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3858 - loss: 2.2513\n",
      "Epoch 15: val_loss did not improve from 1.36131\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m731s\u001b[0m 2s/step - accuracy: 0.3858 - loss: 2.2513 - val_accuracy: 0.6086 - val_loss: 1.4343 - learning_rate: 1.0000e-06\n",
      "Epoch 16/18\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3897 - loss: 2.2720\n",
      "Epoch 16: val_loss did not improve from 1.36131\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m711s\u001b[0m 2s/step - accuracy: 0.3897 - loss: 2.2720 - val_accuracy: 0.6266 - val_loss: 1.3699 - learning_rate: 1.0000e-06\n",
      "Epoch 17/18\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3804 - loss: 2.2874\n",
      "Epoch 17: val_loss did not improve from 1.36131\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 4.999999987376214e-07.\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m716s\u001b[0m 2s/step - accuracy: 0.3804 - loss: 2.2874 - val_accuracy: 0.6061 - val_loss: 1.4033 - learning_rate: 1.0000e-06\n",
      "Epoch 18/18\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4039 - loss: 2.2501\n",
      "Epoch 18: val_loss did not improve from 1.36131\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m658s\u001b[0m 2s/step - accuracy: 0.4039 - loss: 2.2501 - val_accuracy: 0.6053 - val_loss: 1.4228 - learning_rate: 5.0000e-07\n",
      "Restoring model weights from the end of the best epoch: 14.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the previously saved model\n",
    "trmodel = load_model('best_transfer_finetune_model.keras')\n",
    "\n",
    "# Unfreeze only the last few layers of MobileNet for fine-tuning\n",
    "# Assuming 'mobilenet' is part of the model\n",
    "mobilenet = trmodel.get_layer('mobilenetv2_1.00_224')  # Change this if needed\n",
    "\n",
    "# Unfreeze the last few layers\n",
    "for layer in mobilenet.layers[-15:]:  # Adjust number of layers to unfreeze\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model again after unfreezing layers\n",
    "trmodel.compile(\n",
    "    optimizer=Adam(learning_rate=1e-6),  # You may want to lower the learning rate for fine-tuning\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "# Continue training the model\n",
    "history = trmodel.fit(\n",
    "    normalized_train,  # Your training dataset\n",
    "    epochs=18,  # Continue training for 10 more epochs\n",
    "     initial_epoch=8,\n",
    "    validation_data=normalized_validation,  # Your validation dataset\n",
    "    callbacks=[early_stopping, model_checkpoint, lr_schedule]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae9fce62-370b-4d5a-b35d-b88f78a9b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the latest checkpoint\n",
    "trmodelcontinue = load_model('best_transfer_finetune_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0de1dcc-5df6-47c6-acaf-650de4166080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3857 - loss: 2.2903\n",
      "Epoch 1: val_loss improved from inf to 1.39845, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 2s/step - accuracy: 0.3857 - loss: 2.2903 - val_accuracy: 0.6176 - val_loss: 1.3985 - learning_rate: 1.0000e-06\n",
      "Epoch 2/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3888 - loss: 2.2614\n",
      "Epoch 2: val_loss did not improve from 1.39845\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m781s\u001b[0m 2s/step - accuracy: 0.3888 - loss: 2.2613 - val_accuracy: 0.6204 - val_loss: 1.4144 - learning_rate: 1.0000e-06\n",
      "Epoch 3/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3810 - loss: 2.2599\n",
      "Epoch 3: val_loss did not improve from 1.39845\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m732s\u001b[0m 2s/step - accuracy: 0.3810 - loss: 2.2599 - val_accuracy: 0.6020 - val_loss: 1.4131 - learning_rate: 1.0000e-06\n",
      "Epoch 4/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3845 - loss: 2.2826\n",
      "Epoch 4: val_loss improved from 1.39845 to 1.35302, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m720s\u001b[0m 2s/step - accuracy: 0.3845 - loss: 2.2825 - val_accuracy: 0.6201 - val_loss: 1.3530 - learning_rate: 1.0000e-06\n",
      "Epoch 5/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3888 - loss: 2.2491\n",
      "Epoch 5: val_loss did not improve from 1.35302\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m737s\u001b[0m 2s/step - accuracy: 0.3888 - loss: 2.2492 - val_accuracy: 0.6184 - val_loss: 1.4104 - learning_rate: 1.0000e-06\n",
      "Epoch 6/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3858 - loss: 2.2781\n",
      "Epoch 6: val_loss improved from 1.35302 to 1.34934, saving model to best_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m707s\u001b[0m 2s/step - accuracy: 0.3858 - loss: 2.2781 - val_accuracy: 0.6365 - val_loss: 1.3493 - learning_rate: 1.0000e-06\n",
      "Epoch 7/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3853 - loss: 2.2618\n",
      "Epoch 7: val_loss did not improve from 1.34934\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m704s\u001b[0m 2s/step - accuracy: 0.3853 - loss: 2.2618 - val_accuracy: 0.6154 - val_loss: 1.4097 - learning_rate: 1.0000e-06\n",
      "Epoch 8/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4016 - loss: 2.2362\n",
      "Epoch 8: val_loss did not improve from 1.34934\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m707s\u001b[0m 2s/step - accuracy: 0.4016 - loss: 2.2362 - val_accuracy: 0.6110 - val_loss: 1.3934 - learning_rate: 1.0000e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3911 - loss: 2.2565\n",
      "Epoch 9: val_loss did not improve from 1.34934\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m711s\u001b[0m 2s/step - accuracy: 0.3911 - loss: 2.2565 - val_accuracy: 0.6129 - val_loss: 1.4508 - learning_rate: 1.0000e-06\n",
      "Epoch 10/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3821 - loss: 2.2684\n",
      "Epoch 10: val_loss did not improve from 1.34934\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.999999987376214e-07.\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m706s\u001b[0m 2s/step - accuracy: 0.3821 - loss: 2.2684 - val_accuracy: 0.6003 - val_loss: 1.4136 - learning_rate: 1.0000e-06\n",
      "Epoch 11/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95s/step - accuracy: 0.3894 - loss: 2.2946  \n",
      "Epoch 11: val_loss did not improve from 1.34934\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28135s\u001b[0m 95s/step - accuracy: 0.3894 - loss: 2.2945 - val_accuracy: 0.6406 - val_loss: 1.3677 - learning_rate: 5.0000e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3917 - loss: 2.2506\n",
      "Epoch 12: val_loss did not improve from 1.34934\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 1s/step - accuracy: 0.3917 - loss: 2.2506 - val_accuracy: 0.6184 - val_loss: 1.3557 - learning_rate: 5.0000e-07\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "history = trmodelcontinue.fit( \n",
    "    normalized_train,\n",
    "    epochs=20,  # Total 10 more)\n",
    "    validation_data=normalized_validation,\n",
    "    callbacks=[early_stopping, model_checkpoint, lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0756ec00-f115-4212-96db-a7ab3cc40790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3639 - loss: 2.4251\n",
      "Epoch 1: val_accuracy improved from -inf to 0.61349, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m598s\u001b[0m 2s/step - accuracy: 0.3639 - loss: 2.4251 - val_accuracy: 0.6135 - val_loss: 1.3985 - learning_rate: 1.0000e-05\n",
      "Epoch 2/5\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3669 - loss: 2.3517\n",
      "Epoch 2: val_accuracy improved from 0.61349 to 0.63211, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 2s/step - accuracy: 0.3670 - loss: 2.3516 - val_accuracy: 0.6321 - val_loss: 1.3343 - learning_rate: 1.0000e-05\n",
      "Epoch 3/5\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3864 - loss: 2.2771\n",
      "Epoch 3: val_accuracy improved from 0.63211 to 0.64062, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 2s/step - accuracy: 0.3864 - loss: 2.2771 - val_accuracy: 0.6406 - val_loss: 1.3212 - learning_rate: 1.0000e-05\n",
      "Epoch 4/5\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3926 - loss: 2.2813\n",
      "Epoch 4: val_accuracy did not improve from 0.64062\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m512s\u001b[0m 2s/step - accuracy: 0.3925 - loss: 2.2813 - val_accuracy: 0.6209 - val_loss: 1.3656 - learning_rate: 1.0000e-05\n",
      "Epoch 5/5\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3902 - loss: 2.2571\n",
      "Epoch 5: val_accuracy did not improve from 0.64062\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m582s\u001b[0m 2s/step - accuracy: 0.3902 - loss: 2.2571 - val_accuracy: 0.6266 - val_loss: 1.4015 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 3.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the previously saved model\n",
    "trmodel = load_model('best_transfer_finetune_model.keras')\n",
    "\n",
    "# Unfreeze only the last few layers of MobileNet for fine-tuning\n",
    "# Assuming 'mobilenet' is part of the model\n",
    "mobilenet = trmodel.get_layer('mobilenetv2_1.00_224')  # Change this if needed\n",
    "\n",
    "# Unfreeze the last few layers\n",
    "for layer in mobilenet.layers[-20:]:  # Adjust number of layers to unfreeze\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model again after unfreezing layers\n",
    "trmodel.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),  # You may want to lower the learning rate for fine-tuning\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "# Continue training the model\n",
    "history = trmodel.fit(\n",
    "    normalized_train, \n",
    "    epochs=5, \n",
    "    validation_data=normalized_validation, \n",
    "    callbacks=[early_stopping, model_checkpoint, lr_schedule]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04f2f445-29f1-4628-ab18-e32f18f459e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3827 - loss: 2.2748\n",
      "Epoch 1: val_loss improved from inf to 1.34373, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.61595, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m553s\u001b[0m 2s/step - accuracy: 0.3827 - loss: 2.2749 - val_accuracy: 0.6160 - val_loss: 1.3437 - learning_rate: 1.0000e-05\n",
      "Epoch 2/5\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3914 - loss: 2.2701\n",
      "Epoch 2: val_loss did not improve from 1.34373\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.61595 to 0.64047, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 2s/step - accuracy: 0.3914 - loss: 2.2701 - val_accuracy: 0.6405 - val_loss: 1.3690 - learning_rate: 1.0000e-05\n",
      "Epoch 3/5\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3920 - loss: 2.2595\n",
      "Epoch 3: val_loss did not improve from 1.34373\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.64047\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 2s/step - accuracy: 0.3920 - loss: 2.2595 - val_accuracy: 0.6266 - val_loss: 1.3446 - learning_rate: 1.0000e-05\n",
      "Epoch 4/5\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3884 - loss: 2.2680\n",
      "Epoch 4: val_loss did not improve from 1.34373\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.64047\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 2s/step - accuracy: 0.3884 - loss: 2.2680 - val_accuracy: 0.6246 - val_loss: 1.3628 - learning_rate: 1.0000e-05\n",
      "Epoch 5/5\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4041 - loss: 2.2079\n",
      "Epoch 5: val_loss improved from 1.34373 to 1.29388, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.64047\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 2s/step - accuracy: 0.4041 - loss: 2.2078 - val_accuracy: 0.6346 - val_loss: 1.2939 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 5.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint2 = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "trmodelcontinue = load_model('bestaccuracy_transfer_finetune_model.keras')\n",
    "# Continue training the model\n",
    "history = trmodelcontinue.fit(\n",
    "    normalized_train, \n",
    "    epochs=5, \n",
    "    validation_data=normalized_validation, \n",
    "    callbacks=[early_stopping, model_checkpoint,model_checkpoint2, lr_schedule]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b6963ac-706d-4662-99b3-58e3f323d322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3994 - loss: 2.2331\n",
      "Epoch 1: val_loss improved from inf to 1.35271, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.64062, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m740s\u001b[0m 2s/step - accuracy: 0.3994 - loss: 2.2332 - val_accuracy: 0.6406 - val_loss: 1.3527 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3947 - loss: 2.2383\n",
      "Epoch 2: val_loss did not improve from 1.35271\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.64062\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m731s\u001b[0m 2s/step - accuracy: 0.3948 - loss: 2.2383 - val_accuracy: 0.6279 - val_loss: 1.3533 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4086 - loss: 2.2146\n",
      "Epoch 3: val_loss improved from 1.35271 to 1.33987, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.64062\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m744s\u001b[0m 2s/step - accuracy: 0.4086 - loss: 2.2146 - val_accuracy: 0.6242 - val_loss: 1.3399 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4091 - loss: 2.2000\n",
      "Epoch 4: val_loss improved from 1.33987 to 1.32476, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.64062\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 2s/step - accuracy: 0.4090 - loss: 2.2000 - val_accuracy: 0.6332 - val_loss: 1.3248 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3891 - loss: 2.2572\n",
      "Epoch 5: val_loss improved from 1.32476 to 1.29823, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.64062 to 0.64474, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m744s\u001b[0m 2s/step - accuracy: 0.3891 - loss: 2.2571 - val_accuracy: 0.6447 - val_loss: 1.2982 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3967 - loss: 2.2139\n",
      "Epoch 6: val_loss did not improve from 1.29823\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.64474\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m816s\u001b[0m 3s/step - accuracy: 0.3967 - loss: 2.2138 - val_accuracy: 0.6373 - val_loss: 1.3240 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4013 - loss: 2.1454\n",
      "Epoch 7: val_loss did not improve from 1.29823\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.64474\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m736s\u001b[0m 2s/step - accuracy: 0.4013 - loss: 2.1456 - val_accuracy: 0.6271 - val_loss: 1.4089 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4112 - loss: 2.1650\n",
      "Epoch 8: val_loss did not improve from 1.29823\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.64474\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m726s\u001b[0m 2s/step - accuracy: 0.4112 - loss: 2.1650 - val_accuracy: 0.6447 - val_loss: 1.3320 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3999 - loss: 2.1643\n",
      "Epoch 9: val_loss did not improve from 1.29823\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.64474\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 1s/step - accuracy: 0.3999 - loss: 2.1642 - val_accuracy: 0.6120 - val_loss: 1.3662 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4125 - loss: 2.1422\n",
      "Epoch 10: val_loss improved from 1.29823 to 1.29293, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.64474 to 0.66388, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 1s/step - accuracy: 0.4125 - loss: 2.1422 - val_accuracy: 0.6639 - val_loss: 1.2929 - learning_rate: 5.0000e-06\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint2 = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "trmodelcontinue = load_model('bestaccuracy_transfer_finetune_model.keras')\n",
    "# Continue training the model\n",
    "history = trmodelcontinue.fit(\n",
    "    normalized_train, \n",
    "    epochs=10, \n",
    "    validation_data=normalized_validation, \n",
    "    callbacks=[early_stopping, model_checkpoint,model_checkpoint2, lr_schedule]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed7d4b59-cd29-44dc-92d7-2477fbd5019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4133 - loss: 2.1411\n",
      "Epoch 1: val_loss improved from inf to 1.26712, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.67023, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 1s/step - accuracy: 0.4133 - loss: 2.1411 - val_accuracy: 0.6702 - val_loss: 1.2671 - learning_rate: 5.0000e-06\n",
      "Epoch 2/8\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4209 - loss: 2.1225\n",
      "Epoch 2: val_loss did not improve from 1.26712\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.67023\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 1s/step - accuracy: 0.4209 - loss: 2.1225 - val_accuracy: 0.6291 - val_loss: 1.3538 - learning_rate: 5.0000e-06\n",
      "Epoch 3/8\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4228 - loss: 2.0975\n",
      "Epoch 3: val_loss improved from 1.26712 to 1.22449, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.67023\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 1s/step - accuracy: 0.4228 - loss: 2.0976 - val_accuracy: 0.6571 - val_loss: 1.2245 - learning_rate: 5.0000e-06\n",
      "Epoch 4/8\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4170 - loss: 2.1376\n",
      "Epoch 4: val_loss did not improve from 1.22449\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.67023\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 1s/step - accuracy: 0.4170 - loss: 2.1376 - val_accuracy: 0.6587 - val_loss: 1.2822 - learning_rate: 5.0000e-06\n",
      "Epoch 5/8\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4255 - loss: 2.0979\n",
      "Epoch 5: val_loss did not improve from 1.22449\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.67023\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 1s/step - accuracy: 0.4255 - loss: 2.0979 - val_accuracy: 0.6365 - val_loss: 1.3529 - learning_rate: 5.0000e-06\n",
      "Epoch 6/8\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4060 - loss: 2.1128\n",
      "Epoch 6: val_loss did not improve from 1.22449\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.67023\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m470s\u001b[0m 1s/step - accuracy: 0.4060 - loss: 2.1128 - val_accuracy: 0.6678 - val_loss: 1.2891 - learning_rate: 5.0000e-06\n",
      "Epoch 7/8\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4195 - loss: 2.0994\n",
      "Epoch 7: val_loss did not improve from 1.22449\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.67023\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 1s/step - accuracy: 0.4195 - loss: 2.0994 - val_accuracy: 0.6595 - val_loss: 1.3130 - learning_rate: 5.0000e-06\n",
      "Epoch 8/8\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4164 - loss: 2.1263\n",
      "Epoch 8: val_loss did not improve from 1.22449\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.67023\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 1s/step - accuracy: 0.4164 - loss: 2.1264 - val_accuracy: 0.6357 - val_loss: 1.2905 - learning_rate: 2.5000e-06\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint2 = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "trmodelcontinue = load_model('bestaccuracy_transfer_finetune_model.keras')\n",
    "# Continue training the model\n",
    "history = trmodelcontinue.fit(\n",
    "    normalized_train, \n",
    "    epochs=8, \n",
    "    validation_data=normalized_validation, \n",
    "    callbacks=[early_stopping, model_checkpoint,model_checkpoint2, lr_schedule]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4b8941f-3da3-4d26-be7f-dc6a81e692e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4783 - loss: 1.8622\n",
      "Epoch 1: val_loss improved from inf to 1.10235, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69079, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 1s/step - accuracy: 0.4783 - loss: 1.8622 - val_accuracy: 0.6908 - val_loss: 1.1024 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4902 - loss: 1.8022\n",
      "Epoch 2: val_loss did not improve from 1.10235\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.69079 to 0.69231, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 1s/step - accuracy: 0.4901 - loss: 1.8023 - val_accuracy: 0.6923 - val_loss: 1.1194 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4921 - loss: 1.7955\n",
      "Epoch 3: val_loss improved from 1.10235 to 1.05842, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.69231 to 0.69984, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 1s/step - accuracy: 0.4921 - loss: 1.7954 - val_accuracy: 0.6998 - val_loss: 1.0584 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5211 - loss: 1.6949\n",
      "Epoch 4: val_loss improved from 1.05842 to 1.02224, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.69984 to 0.71628, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 1s/step - accuracy: 0.5211 - loss: 1.6950 - val_accuracy: 0.7163 - val_loss: 1.0222 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5212 - loss: 1.7131\n",
      "Epoch 5: val_loss did not improve from 1.02224\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.71628\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 1s/step - accuracy: 0.5211 - loss: 1.7131 - val_accuracy: 0.6801 - val_loss: 1.0391 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5190 - loss: 1.6847\n",
      "Epoch 6: val_loss did not improve from 1.02224\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.71628\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 1s/step - accuracy: 0.5190 - loss: 1.6848 - val_accuracy: 0.6998 - val_loss: 1.0720 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5182 - loss: 1.6832\n",
      "Epoch 7: val_loss did not improve from 1.02224\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.71628\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 1s/step - accuracy: 0.5182 - loss: 1.6833 - val_accuracy: 0.7007 - val_loss: 1.0553 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5320 - loss: 1.6385\n",
      "Epoch 8: val_loss improved from 1.02224 to 0.94386, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.71628 to 0.72944, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 1s/step - accuracy: 0.5320 - loss: 1.6386 - val_accuracy: 0.7294 - val_loss: 0.9439 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5398 - loss: 1.5959\n",
      "Epoch 9: val_loss improved from 0.94386 to 0.85653, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.72944 to 0.75167, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 1s/step - accuracy: 0.5398 - loss: 1.5959 - val_accuracy: 0.7517 - val_loss: 0.8565 - learning_rate: 1.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5365 - loss: 1.5905\n",
      "Epoch 10: val_loss did not improve from 0.85653\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.75167\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 1s/step - accuracy: 0.5365 - loss: 1.5905 - val_accuracy: 0.7249 - val_loss: 0.9284 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 9.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the previously saved model\n",
    "trmodel = load_model('bestaccuracy_transfer_finetune_model.keras')\n",
    "\n",
    "# Unfreeze only the last few layers of MobileNet for fine-tuning\n",
    "# Assuming 'mobilenet' is part of the model\n",
    "mobilenet = trmodel.get_layer('mobilenetv2_1.00_224')  # Change this if needed\n",
    "\n",
    "# Unfreeze the last few layers\n",
    "for layer in mobilenet.layers[-25:]:  # Adjust number of layers to unfreeze\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model again after unfreezing layers\n",
    "trmodel.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),  # You may want to lower the learning rate for fine-tuning\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint2 = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "# Continue training the model\n",
    "history = trmodel.fit(\n",
    "    normalized_train,  # Your training dataset\n",
    "    epochs=10,  # Continue training for 10 more epochs\n",
    "    validation_data=normalized_validation,  # Your validation dataset\n",
    "    callbacks=[early_stopping, model_checkpoint, model_checkpoint2, lr_schedule]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f857b7a7-86ec-40ac-8325-b845eadc616b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5465 - loss: 1.5769\n",
      "Epoch 11: val_loss improved from inf to 0.92320, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from -inf to 0.72039, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 1s/step - accuracy: 0.5465 - loss: 1.5770 - val_accuracy: 0.7204 - val_loss: 0.9232 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5429 - loss: 1.6023\n",
      "Epoch 12: val_loss improved from 0.92320 to 0.88972, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.72039 to 0.73411, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m665s\u001b[0m 2s/step - accuracy: 0.5429 - loss: 1.6022 - val_accuracy: 0.7341 - val_loss: 0.8897 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5553 - loss: 1.5084\n",
      "Epoch 13: val_loss did not improve from 0.88972\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.73411\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m747s\u001b[0m 2s/step - accuracy: 0.5553 - loss: 1.5085 - val_accuracy: 0.7327 - val_loss: 0.8988 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5588 - loss: 1.5414\n",
      "Epoch 14: val_loss improved from 0.88972 to 0.88078, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.73411 to 0.73766, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m736s\u001b[0m 2s/step - accuracy: 0.5588 - loss: 1.5413 - val_accuracy: 0.7377 - val_loss: 0.8808 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5803 - loss: 1.4757\n",
      "Epoch 15: val_loss improved from 0.88078 to 0.84844, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.73766 to 0.74424, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m775s\u001b[0m 2s/step - accuracy: 0.5803 - loss: 1.4757 - val_accuracy: 0.7442 - val_loss: 0.8484 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5674 - loss: 1.4762\n",
      "Epoch 16: val_loss improved from 0.84844 to 0.81890, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.74424 to 0.77220, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 2s/step - accuracy: 0.5675 - loss: 1.4762 - val_accuracy: 0.7722 - val_loss: 0.8189 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5806 - loss: 1.4351\n",
      "Epoch 17: val_loss improved from 0.81890 to 0.81318, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.77220\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m746s\u001b[0m 2s/step - accuracy: 0.5806 - loss: 1.4351 - val_accuracy: 0.7475 - val_loss: 0.8132 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6028 - loss: 1.3668\n",
      "Epoch 18: val_loss improved from 0.81318 to 0.79960, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.77220\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m800s\u001b[0m 2s/step - accuracy: 0.6027 - loss: 1.3669 - val_accuracy: 0.7648 - val_loss: 0.7996 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5939 - loss: 1.3671\n",
      "Epoch 19: val_loss improved from 0.79960 to 0.74523, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.77220 to 0.80184, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m741s\u001b[0m 2s/step - accuracy: 0.5939 - loss: 1.3672 - val_accuracy: 0.8018 - val_loss: 0.7452 - learning_rate: 1.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5809 - loss: 1.4055\n",
      "Epoch 20: val_loss did not improve from 0.74523\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.80184\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 2s/step - accuracy: 0.5810 - loss: 1.4054 - val_accuracy: 0.7893 - val_loss: 0.7607 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 19.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the previously saved model\n",
    "trmodel = load_model('bestaccuracy_transfer_finetune_model.keras')\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint2 = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "# Continue training the model\n",
    "history = trmodel.fit(\n",
    "    normalized_train,  # Your training dataset\n",
    "    epochs=20,\n",
    "    initial_epoch=10,# Continue training for 10 more epochs\n",
    "    validation_data=normalized_validation,  # Your validation dataset\n",
    "    callbacks=[early_stopping, model_checkpoint, model_checkpoint2, lr_schedule]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bea736b-3ce7-4984-839a-18e13cea2c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5877 - loss: 1.4019\n",
      "Epoch 21: val_loss improved from inf to 0.70889, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from -inf to 0.80016, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 2s/step - accuracy: 0.5877 - loss: 1.4018 - val_accuracy: 0.8002 - val_loss: 0.7089 - learning_rate: 1.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5981 - loss: 1.3598\n",
      "Epoch 22: val_loss did not improve from 0.70889\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.80016\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m791s\u001b[0m 2s/step - accuracy: 0.5981 - loss: 1.3598 - val_accuracy: 0.7701 - val_loss: 0.8072 - learning_rate: 1.0000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5976 - loss: 1.3530\n",
      "Epoch 23: val_loss improved from 0.70889 to 0.70051, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.80016\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m787s\u001b[0m 2s/step - accuracy: 0.5976 - loss: 1.3530 - val_accuracy: 0.7985 - val_loss: 0.7005 - learning_rate: 1.0000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6068 - loss: 1.3529\n",
      "Epoch 24: val_loss did not improve from 0.70051\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.80016\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m739s\u001b[0m 2s/step - accuracy: 0.6068 - loss: 1.3528 - val_accuracy: 0.7952 - val_loss: 0.7158 - learning_rate: 1.0000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6098 - loss: 1.3283\n",
      "Epoch 25: val_loss did not improve from 0.70051\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.80016\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m727s\u001b[0m 2s/step - accuracy: 0.6098 - loss: 1.3282 - val_accuracy: 0.7771 - val_loss: 0.7401 - learning_rate: 1.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6223 - loss: 1.3017\n",
      "Epoch 26: val_loss improved from 0.70051 to 0.68973, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.80016 to 0.80181, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m751s\u001b[0m 2s/step - accuracy: 0.6223 - loss: 1.3017 - val_accuracy: 0.8018 - val_loss: 0.6897 - learning_rate: 1.0000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6438 - loss: 1.2338\n",
      "Epoch 27: val_loss improved from 0.68973 to 0.68694, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.80181\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 2s/step - accuracy: 0.6437 - loss: 1.2338 - val_accuracy: 0.7960 - val_loss: 0.6869 - learning_rate: 1.0000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6305 - loss: 1.2322\n",
      "Epoch 28: val_loss improved from 0.68694 to 0.66431, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 28: val_accuracy improved from 0.80181 to 0.81086, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m721s\u001b[0m 2s/step - accuracy: 0.6304 - loss: 1.2323 - val_accuracy: 0.8109 - val_loss: 0.6643 - learning_rate: 1.0000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6397 - loss: 1.2213\n",
      "Epoch 29: val_loss improved from 0.66431 to 0.64003, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.81086\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m745s\u001b[0m 2s/step - accuracy: 0.6397 - loss: 1.2213 - val_accuracy: 0.8085 - val_loss: 0.6400 - learning_rate: 1.0000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6445 - loss: 1.1969\n",
      "Epoch 30: val_loss improved from 0.64003 to 0.56741, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.81086 to 0.83361, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m722s\u001b[0m 2s/step - accuracy: 0.6445 - loss: 1.1969 - val_accuracy: 0.8336 - val_loss: 0.5674 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the previously saved model\n",
    "trmodel = load_model('bestaccuracy_transfer_finetune_model.keras')\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint2 = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "# Continue training the model\n",
    "history = trmodel.fit(\n",
    "    normalized_train,  # Your training dataset\n",
    "    epochs=30,\n",
    "    initial_epoch=20,# Continue training for 10 more epochs\n",
    "    validation_data=normalized_validation,  # Your validation dataset\n",
    "    callbacks=[early_stopping, model_checkpoint, model_checkpoint2, lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5e9a62b-7308-4f7a-9017-6cb986cee3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6322 - loss: 1.2068\n",
      "Epoch 31: val_loss improved from inf to 0.61749, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 31: val_accuracy improved from -inf to 0.82566, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m748s\u001b[0m 2s/step - accuracy: 0.6322 - loss: 1.2068 - val_accuracy: 0.8257 - val_loss: 0.6175 - learning_rate: 1.0000e-04\n",
      "Epoch 32/40\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6505 - loss: 1.1775\n",
      "Epoch 32: val_loss improved from 0.61749 to 0.59960, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 32: val_accuracy improved from 0.82566 to 0.82860, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m729s\u001b[0m 2s/step - accuracy: 0.6505 - loss: 1.1775 - val_accuracy: 0.8286 - val_loss: 0.5996 - learning_rate: 1.0000e-04\n",
      "Epoch 33/40\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6485 - loss: 1.1849\n",
      "Epoch 33: val_loss did not improve from 0.59960\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.82860\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m731s\u001b[0m 2s/step - accuracy: 0.6485 - loss: 1.1849 - val_accuracy: 0.8117 - val_loss: 0.6170 - learning_rate: 1.0000e-04\n",
      "Epoch 34/40\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6586 - loss: 1.1345\n",
      "Epoch 34: val_loss improved from 0.59960 to 0.55458, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 34: val_accuracy improved from 0.82860 to 0.83141, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 2s/step - accuracy: 0.6586 - loss: 1.1345 - val_accuracy: 0.8314 - val_loss: 0.5546 - learning_rate: 1.0000e-04\n",
      "Epoch 35/40\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6710 - loss: 1.1190\n",
      "Epoch 35: val_loss did not improve from 0.55458\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.83141\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m723s\u001b[0m 2s/step - accuracy: 0.6710 - loss: 1.1191 - val_accuracy: 0.8125 - val_loss: 0.6138 - learning_rate: 1.0000e-04\n",
      "Epoch 36/40\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6689 - loss: 1.0979\n",
      "Epoch 36: val_loss did not improve from 0.55458\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.83141\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m743s\u001b[0m 2s/step - accuracy: 0.6689 - loss: 1.0980 - val_accuracy: 0.8166 - val_loss: 0.5823 - learning_rate: 1.0000e-04\n",
      "Epoch 37/40\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6544 - loss: 1.1262\n",
      "Epoch 37: val_loss did not improve from 0.55458\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.83141\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m780s\u001b[0m 2s/step - accuracy: 0.6545 - loss: 1.1262 - val_accuracy: 0.8161 - val_loss: 0.6028 - learning_rate: 1.0000e-04\n",
      "Epoch 38/40\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6833 - loss: 1.0665\n",
      "Epoch 38: val_loss improved from 0.55458 to 0.53115, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 38: val_accuracy improved from 0.83141 to 0.84046, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m801s\u001b[0m 2s/step - accuracy: 0.6833 - loss: 1.0665 - val_accuracy: 0.8405 - val_loss: 0.5311 - learning_rate: 1.0000e-04\n",
      "Epoch 39/40\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6847 - loss: 1.0459\n",
      "Epoch 39: val_loss did not improve from 0.53115\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.84046\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m728s\u001b[0m 2s/step - accuracy: 0.6847 - loss: 1.0460 - val_accuracy: 0.8018 - val_loss: 0.6158 - learning_rate: 1.0000e-04\n",
      "Epoch 40/40\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6812 - loss: 1.0714\n",
      "Epoch 40: val_loss did not improve from 0.53115\n",
      "\n",
      "Epoch 40: val_accuracy improved from 0.84046 to 0.84114, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m780s\u001b[0m 2s/step - accuracy: 0.6812 - loss: 1.0714 - val_accuracy: 0.8411 - val_loss: 0.5493 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 38.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the previously saved model\n",
    "trmodel = load_model('bestaccuracy_transfer_finetune_model.keras')\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint2 = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "# Continue training the model\n",
    "history = trmodel.fit(\n",
    "    normalized_train,  # Your training dataset\n",
    "    epochs=40,\n",
    "    initial_epoch=30,# Continue training for 10 more epochs\n",
    "    validation_data=normalized_validation,  # Your validation dataset\n",
    "    callbacks=[early_stopping, model_checkpoint, model_checkpoint2, lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c259c86-0183-4d2b-8247-f6a23ca315b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6905 - loss: 1.0271\n",
      "Epoch 41: val_loss improved from inf to 0.52223, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 41: val_accuracy improved from -inf to 0.84786, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 2s/step - accuracy: 0.6905 - loss: 1.0271 - val_accuracy: 0.8479 - val_loss: 0.5222 - learning_rate: 1.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6926 - loss: 1.0337\n",
      "Epoch 42: val_loss did not improve from 0.52223\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.84786\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 2s/step - accuracy: 0.6926 - loss: 1.0338 - val_accuracy: 0.8361 - val_loss: 0.5233 - learning_rate: 1.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6915 - loss: 1.0204\n",
      "Epoch 43: val_loss did not improve from 0.52223\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.84786\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 2s/step - accuracy: 0.6915 - loss: 1.0204 - val_accuracy: 0.8199 - val_loss: 0.5739 - learning_rate: 1.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6969 - loss: 1.0163\n",
      "Epoch 44: val_loss did not improve from 0.52223\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.84786\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m746s\u001b[0m 2s/step - accuracy: 0.6969 - loss: 1.0163 - val_accuracy: 0.8421 - val_loss: 0.5424 - learning_rate: 1.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6922 - loss: 1.0137\n",
      "Epoch 45: val_loss improved from 0.52223 to 0.48265, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 45: val_accuracy improved from 0.84786 to 0.86102, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m659s\u001b[0m 2s/step - accuracy: 0.6922 - loss: 1.0137 - val_accuracy: 0.8610 - val_loss: 0.4826 - learning_rate: 1.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7074 - loss: 0.9716\n",
      "Epoch 46: val_loss improved from 0.48265 to 0.46801, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 46: val_accuracy improved from 0.86102 to 0.86513, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 2s/step - accuracy: 0.7073 - loss: 0.9716 - val_accuracy: 0.8651 - val_loss: 0.4680 - learning_rate: 1.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7155 - loss: 0.9459\n",
      "Epoch 47: val_loss did not improve from 0.46801\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.86513\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 2s/step - accuracy: 0.7155 - loss: 0.9460 - val_accuracy: 0.8604 - val_loss: 0.4774 - learning_rate: 1.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7077 - loss: 0.9530\n",
      "Epoch 48: val_loss did not improve from 0.46801\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.86513\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m719s\u001b[0m 2s/step - accuracy: 0.7077 - loss: 0.9531 - val_accuracy: 0.8594 - val_loss: 0.4765 - learning_rate: 1.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7109 - loss: 0.9446\n",
      "Epoch 49: val_loss did not improve from 0.46801\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.86513\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m717s\u001b[0m 2s/step - accuracy: 0.7109 - loss: 0.9446 - val_accuracy: 0.8562 - val_loss: 0.5021 - learning_rate: 1.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7208 - loss: 0.9094\n",
      "Epoch 50: val_loss improved from 0.46801 to 0.44626, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.86513\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 2s/step - accuracy: 0.7208 - loss: 0.9094 - val_accuracy: 0.8562 - val_loss: 0.4463 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 50.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the previously saved model\n",
    "trmodel = load_model('bestaccuracy_transfer_finetune_model.keras')\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint2 = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "# Continue training the model\n",
    "history = trmodel.fit(\n",
    "    normalized_train,  # Your training dataset\n",
    "    epochs=50,\n",
    "    initial_epoch=40,# Continue training for 10 more epochs\n",
    "    validation_data=normalized_validation,  # Your validation dataset\n",
    "    callbacks=[early_stopping, model_checkpoint, model_checkpoint2, lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94f9fc32-db69-4cfc-a9c9-a43741b4e0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/60\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7041 - loss: 0.9833\n",
      "Epoch 51: val_loss improved from inf to 0.52258, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 51: val_accuracy improved from -inf to 0.84457, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 2s/step - accuracy: 0.7041 - loss: 0.9833 - val_accuracy: 0.8446 - val_loss: 0.5226 - learning_rate: 1.0000e-04\n",
      "Epoch 52/60\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7149 - loss: 0.9371\n",
      "Epoch 52: val_loss did not improve from 0.52258\n",
      "\n",
      "Epoch 52: val_accuracy did not improve from 0.84457\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 2s/step - accuracy: 0.7149 - loss: 0.9371 - val_accuracy: 0.8420 - val_loss: 0.5255 - learning_rate: 1.0000e-04\n",
      "Epoch 53/60\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7158 - loss: 0.9438\n",
      "Epoch 53: val_loss improved from 0.52258 to 0.51964, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 53: val_accuracy improved from 0.84457 to 0.85280, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 2s/step - accuracy: 0.7157 - loss: 0.9439 - val_accuracy: 0.8528 - val_loss: 0.5196 - learning_rate: 1.0000e-04\n",
      "Epoch 54/60\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7173 - loss: 0.9263\n",
      "Epoch 54: val_loss improved from 0.51964 to 0.48111, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.85280\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m507s\u001b[0m 2s/step - accuracy: 0.7173 - loss: 0.9263 - val_accuracy: 0.8446 - val_loss: 0.4811 - learning_rate: 1.0000e-04\n",
      "Epoch 55/60\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7138 - loss: 0.9229\n",
      "Epoch 55: val_loss improved from 0.48111 to 0.44265, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 55: val_accuracy improved from 0.85280 to 0.85773, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m519s\u001b[0m 2s/step - accuracy: 0.7138 - loss: 0.9229 - val_accuracy: 0.8577 - val_loss: 0.4427 - learning_rate: 1.0000e-04\n",
      "Epoch 56/60\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7322 - loss: 0.8789\n",
      "Epoch 56: val_loss did not improve from 0.44265\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.85773\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 2s/step - accuracy: 0.7322 - loss: 0.8790 - val_accuracy: 0.8454 - val_loss: 0.4805 - learning_rate: 1.0000e-04\n",
      "Epoch 57/60\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7160 - loss: 0.9042\n",
      "Epoch 57: val_loss improved from 0.44265 to 0.43244, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 57: val_accuracy improved from 0.85773 to 0.87207, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 2s/step - accuracy: 0.7160 - loss: 0.9042 - val_accuracy: 0.8721 - val_loss: 0.4324 - learning_rate: 1.0000e-04\n",
      "Epoch 58/60\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7251 - loss: 0.8775\n",
      "Epoch 58: val_loss did not improve from 0.43244\n",
      "\n",
      "Epoch 58: val_accuracy did not improve from 0.87207\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 2s/step - accuracy: 0.7251 - loss: 0.8776 - val_accuracy: 0.8512 - val_loss: 0.4612 - learning_rate: 1.0000e-04\n",
      "Epoch 59/60\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7306 - loss: 0.8662\n",
      "Epoch 59: val_loss did not improve from 0.43244\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.87207\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 2s/step - accuracy: 0.7306 - loss: 0.8663 - val_accuracy: 0.8595 - val_loss: 0.4648 - learning_rate: 1.0000e-04\n",
      "Epoch 60/60\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7437 - loss: 0.8539\n",
      "Epoch 60: val_loss improved from 0.43244 to 0.37704, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 60: val_accuracy improved from 0.87207 to 0.89548, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 2s/step - accuracy: 0.7437 - loss: 0.8539 - val_accuracy: 0.8955 - val_loss: 0.3770 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 60.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the previously saved model\n",
    "trmodel = load_model('bestaccuracy_transfer_finetune_model.keras')\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint2 = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "# Continue training the model\n",
    "history = trmodel.fit(\n",
    "    normalized_train,  # Your training dataset\n",
    "    epochs=60,\n",
    "    initial_epoch=50,# Continue training for 10 more epochs\n",
    "    validation_data=normalized_validation,  # Your validation dataset\n",
    "    callbacks=[early_stopping, model_checkpoint, model_checkpoint2, lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf2ec3c1-37f7-49d7-a79f-ddfe30291b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/70\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7319 - loss: 0.8761\n",
      "Epoch 61: val_loss improved from inf to 0.41534, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 61: val_accuracy improved from -inf to 0.88322, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 2s/step - accuracy: 0.7319 - loss: 0.8761 - val_accuracy: 0.8832 - val_loss: 0.4153 - learning_rate: 1.0000e-04\n",
      "Epoch 62/70\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7523 - loss: 0.8060\n",
      "Epoch 62: val_loss improved from 0.41534 to 0.37357, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 62: val_accuracy improved from 0.88322 to 0.89130, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 2s/step - accuracy: 0.7523 - loss: 0.8061 - val_accuracy: 0.8913 - val_loss: 0.3736 - learning_rate: 1.0000e-04\n",
      "Epoch 63/70\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7424 - loss: 0.8579\n",
      "Epoch 63: val_loss did not improve from 0.37357\n",
      "\n",
      "Epoch 63: val_accuracy did not improve from 0.89130\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m675s\u001b[0m 2s/step - accuracy: 0.7424 - loss: 0.8579 - val_accuracy: 0.8783 - val_loss: 0.4034 - learning_rate: 1.0000e-04\n",
      "Epoch 64/70\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7460 - loss: 0.8081\n",
      "Epoch 64: val_loss did not improve from 0.37357\n",
      "\n",
      "Epoch 64: val_accuracy improved from 0.89130 to 0.89145, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m790s\u001b[0m 2s/step - accuracy: 0.7460 - loss: 0.8081 - val_accuracy: 0.8914 - val_loss: 0.3929 - learning_rate: 1.0000e-04\n",
      "Epoch 65/70\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7603 - loss: 0.8008\n",
      "Epoch 65: val_loss did not improve from 0.37357\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.89145\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m838s\u001b[0m 2s/step - accuracy: 0.7603 - loss: 0.8008 - val_accuracy: 0.8660 - val_loss: 0.4557 - learning_rate: 1.0000e-04\n",
      "Epoch 66/70\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7533 - loss: 0.8009\n",
      "Epoch 66: val_loss did not improve from 0.37357\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.89145\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m750s\u001b[0m 2s/step - accuracy: 0.7533 - loss: 0.8009 - val_accuracy: 0.8775 - val_loss: 0.4176 - learning_rate: 1.0000e-04\n",
      "Epoch 67/70\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7673 - loss: 0.7728\n",
      "Epoch 67: val_loss improved from 0.37357 to 0.34635, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.89145\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m745s\u001b[0m 2s/step - accuracy: 0.7673 - loss: 0.7728 - val_accuracy: 0.8905 - val_loss: 0.3463 - learning_rate: 5.0000e-05\n",
      "Epoch 68/70\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7714 - loss: 0.7385\n",
      "Epoch 68: val_loss did not improve from 0.34635\n",
      "\n",
      "Epoch 68: val_accuracy did not improve from 0.89145\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m728s\u001b[0m 2s/step - accuracy: 0.7714 - loss: 0.7384 - val_accuracy: 0.8783 - val_loss: 0.3833 - learning_rate: 5.0000e-05\n",
      "Epoch 69/70\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7854 - loss: 0.6918\n",
      "Epoch 69: val_loss did not improve from 0.34635\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.89145\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m730s\u001b[0m 2s/step - accuracy: 0.7854 - loss: 0.6919 - val_accuracy: 0.8863 - val_loss: 0.3685 - learning_rate: 5.0000e-05\n",
      "Epoch 70/70\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7904 - loss: 0.6903\n",
      "Epoch 70: val_loss improved from 0.34635 to 0.33956, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 70: val_accuracy improved from 0.89145 to 0.90970, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m776s\u001b[0m 2s/step - accuracy: 0.7904 - loss: 0.6903 - val_accuracy: 0.9097 - val_loss: 0.3396 - learning_rate: 5.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 70.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the previously saved model\n",
    "trmodel = load_model('bestaccuracy_transfer_finetune_model.keras')\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint2 = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "# Continue training the model\n",
    "history = trmodel.fit(\n",
    "    normalized_train,  # Your training dataset\n",
    "    epochs=70,\n",
    "    initial_epoch=60,# Continue training for 10 more epochs\n",
    "    validation_data=normalized_validation,  # Your validation dataset\n",
    "    callbacks=[early_stopping, model_checkpoint, model_checkpoint2, lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32c11575-5466-452a-931e-a1cc0d604c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/80\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7893 - loss: 0.6872\n",
      "Epoch 71: val_loss improved from inf to 0.34321, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 71: val_accuracy improved from -inf to 0.88980, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m748s\u001b[0m 2s/step - accuracy: 0.7893 - loss: 0.6872 - val_accuracy: 0.8898 - val_loss: 0.3432 - learning_rate: 5.0000e-05\n",
      "Epoch 72/80\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7814 - loss: 0.7070\n",
      "Epoch 72: val_loss improved from 0.34321 to 0.30723, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 72: val_accuracy improved from 0.88980 to 0.90970, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m725s\u001b[0m 2s/step - accuracy: 0.7814 - loss: 0.7070 - val_accuracy: 0.9097 - val_loss: 0.3072 - learning_rate: 5.0000e-05\n",
      "Epoch 73/80\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7930 - loss: 0.6795\n",
      "Epoch 73: val_loss did not improve from 0.30723\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.90970\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m717s\u001b[0m 2s/step - accuracy: 0.7930 - loss: 0.6795 - val_accuracy: 0.9062 - val_loss: 0.3211 - learning_rate: 5.0000e-05\n",
      "Epoch 74/80\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7848 - loss: 0.6781\n",
      "Epoch 74: val_loss improved from 0.30723 to 0.30672, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 74: val_accuracy improved from 0.90970 to 0.91036, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m729s\u001b[0m 2s/step - accuracy: 0.7848 - loss: 0.6781 - val_accuracy: 0.9104 - val_loss: 0.3067 - learning_rate: 5.0000e-05\n",
      "Epoch 75/80\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7899 - loss: 0.6740\n",
      "Epoch 75: val_loss improved from 0.30672 to 0.28298, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 75: val_accuracy improved from 0.91036 to 0.91776, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m738s\u001b[0m 2s/step - accuracy: 0.7899 - loss: 0.6740 - val_accuracy: 0.9178 - val_loss: 0.2830 - learning_rate: 5.0000e-05\n",
      "Epoch 76/80\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7984 - loss: 0.6573\n",
      "Epoch 76: val_loss did not improve from 0.28298\n",
      "\n",
      "Epoch 76: val_accuracy did not improve from 0.91776\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m729s\u001b[0m 2s/step - accuracy: 0.7984 - loss: 0.6573 - val_accuracy: 0.9095 - val_loss: 0.3053 - learning_rate: 5.0000e-05\n",
      "Epoch 77/80\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8039 - loss: 0.6413\n",
      "Epoch 77: val_loss did not improve from 0.28298\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.91776\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m756s\u001b[0m 2s/step - accuracy: 0.8039 - loss: 0.6414 - val_accuracy: 0.9130 - val_loss: 0.3038 - learning_rate: 5.0000e-05\n",
      "Epoch 78/80\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7949 - loss: 0.6477\n",
      "Epoch 78: val_loss did not improve from 0.28298\n",
      "\n",
      "Epoch 78: val_accuracy did not improve from 0.91776\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m775s\u001b[0m 2s/step - accuracy: 0.7949 - loss: 0.6477 - val_accuracy: 0.9104 - val_loss: 0.3072 - learning_rate: 5.0000e-05\n",
      "Epoch 79/80\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7994 - loss: 0.6477\n",
      "Epoch 79: val_loss did not improve from 0.28298\n",
      "\n",
      "Epoch 79: val_accuracy did not improve from 0.91776\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m958s\u001b[0m 3s/step - accuracy: 0.7994 - loss: 0.6477 - val_accuracy: 0.9022 - val_loss: 0.3303 - learning_rate: 5.0000e-05\n",
      "Epoch 80/80\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7977 - loss: 0.6233\n",
      "Epoch 80: val_loss improved from 0.28298 to 0.26408, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 80: val_accuracy improved from 0.91776 to 0.92642, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m831s\u001b[0m 3s/step - accuracy: 0.7978 - loss: 0.6233 - val_accuracy: 0.9264 - val_loss: 0.2641 - learning_rate: 2.5000e-05\n",
      "Restoring model weights from the end of the best epoch: 80.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the previously saved model\n",
    "trmodel = load_model('bestaccuracy_transfer_finetune_model.keras')\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint2 = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "# Continue training the model\n",
    "history = trmodel.fit(\n",
    "    normalized_train,  # Your training dataset\n",
    "    epochs=80,\n",
    "    initial_epoch=70,# Continue training for 10 more epochs\n",
    "    validation_data=normalized_validation,  # Your validation dataset\n",
    "    callbacks=[early_stopping, model_checkpoint, model_checkpoint2, lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16fb3813-a026-49ff-95f5-e0cd6cb47e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/90\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8073 - loss: 0.6139\n",
      "Epoch 81: val_loss improved from inf to 0.26393, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 81: val_accuracy improved from -inf to 0.93092, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 2s/step - accuracy: 0.8073 - loss: 0.6139 - val_accuracy: 0.9309 - val_loss: 0.2639 - learning_rate: 2.5000e-05\n",
      "Epoch 82/90\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8116 - loss: 0.5982\n",
      "Epoch 82: val_loss did not improve from 0.26393\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.93092\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m777s\u001b[0m 2s/step - accuracy: 0.8116 - loss: 0.5983 - val_accuracy: 0.9130 - val_loss: 0.2995 - learning_rate: 2.5000e-05\n",
      "Epoch 83/90\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8159 - loss: 0.5873\n",
      "Epoch 83: val_loss did not improve from 0.26393\n",
      "\n",
      "Epoch 83: val_accuracy did not improve from 0.93092\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m745s\u001b[0m 2s/step - accuracy: 0.8159 - loss: 0.5874 - val_accuracy: 0.9219 - val_loss: 0.2749 - learning_rate: 2.5000e-05\n",
      "Epoch 84/90\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8111 - loss: 0.6011\n",
      "Epoch 84: val_loss did not improve from 0.26393\n",
      "\n",
      "Epoch 84: val_accuracy did not improve from 0.93092\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m723s\u001b[0m 2s/step - accuracy: 0.8111 - loss: 0.6012 - val_accuracy: 0.9161 - val_loss: 0.3044 - learning_rate: 2.5000e-05\n",
      "Epoch 85/90\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8081 - loss: 0.6188\n",
      "Epoch 85: val_loss improved from 0.26393 to 0.25931, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 85: val_accuracy did not improve from 0.93092\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m729s\u001b[0m 2s/step - accuracy: 0.8081 - loss: 0.6188 - val_accuracy: 0.9211 - val_loss: 0.2593 - learning_rate: 2.5000e-05\n",
      "Epoch 86/90\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8233 - loss: 0.5812\n",
      "Epoch 86: val_loss did not improve from 0.25931\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.93092\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m727s\u001b[0m 2s/step - accuracy: 0.8233 - loss: 0.5813 - val_accuracy: 0.9219 - val_loss: 0.2690 - learning_rate: 2.5000e-05\n",
      "Epoch 87/90\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8232 - loss: 0.5744\n",
      "Epoch 87: val_loss improved from 0.25931 to 0.23570, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.93092\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m731s\u001b[0m 2s/step - accuracy: 0.8232 - loss: 0.5744 - val_accuracy: 0.9281 - val_loss: 0.2357 - learning_rate: 2.5000e-05\n",
      "Epoch 88/90\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8177 - loss: 0.5810\n",
      "Epoch 88: val_loss did not improve from 0.23570\n",
      "\n",
      "Epoch 88: val_accuracy did not improve from 0.93092\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m726s\u001b[0m 2s/step - accuracy: 0.8177 - loss: 0.5811 - val_accuracy: 0.9211 - val_loss: 0.2671 - learning_rate: 2.5000e-05\n",
      "Epoch 89/90\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8065 - loss: 0.5986\n",
      "Epoch 89: val_loss improved from 0.23570 to 0.23484, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 89: val_accuracy improved from 0.93092 to 0.93729, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m722s\u001b[0m 2s/step - accuracy: 0.8065 - loss: 0.5986 - val_accuracy: 0.9373 - val_loss: 0.2348 - learning_rate: 2.5000e-05\n",
      "Epoch 90/90\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8208 - loss: 0.5874\n",
      "Epoch 90: val_loss did not improve from 0.23484\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.93729\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22914s\u001b[0m 78s/step - accuracy: 0.8208 - loss: 0.5874 - val_accuracy: 0.9130 - val_loss: 0.2741 - learning_rate: 2.5000e-05\n",
      "Restoring model weights from the end of the best epoch: 89.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the previously saved model\n",
    "trmodel = load_model('bestaccuracy_transfer_finetune_model.keras')\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint2 = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "# Continue training the model\n",
    "history = trmodel.fit(\n",
    "    normalized_train,  # Your training dataset\n",
    "    epochs=90,\n",
    "    initial_epoch=80,# Continue training for 10 more epochs\n",
    "    validation_data=normalized_validation,  # Your validation dataset\n",
    "    callbacks=[early_stopping, model_checkpoint, model_checkpoint2, lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ebb06d9-57b0-4223-a22b-8825aaf4bb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8226 - loss: 0.5694\n",
      "Epoch 91: val_loss improved from inf to 0.25718, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 91: val_accuracy improved from -inf to 0.91859, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m776s\u001b[0m 2s/step - accuracy: 0.8226 - loss: 0.5694 - val_accuracy: 0.9186 - val_loss: 0.2572 - learning_rate: 2.5000e-05\n",
      "Epoch 92/100\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8192 - loss: 0.5680\n",
      "Epoch 92: val_loss improved from 0.25718 to 0.24729, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 92: val_accuracy improved from 0.91859 to 0.91890, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m867s\u001b[0m 3s/step - accuracy: 0.8192 - loss: 0.5680 - val_accuracy: 0.9189 - val_loss: 0.2473 - learning_rate: 2.5000e-05\n",
      "Epoch 93/100\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8308 - loss: 0.5582\n",
      "Epoch 93: val_loss did not improve from 0.24729\n",
      "\n",
      "Epoch 93: val_accuracy improved from 0.91890 to 0.92270, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m728s\u001b[0m 2s/step - accuracy: 0.8308 - loss: 0.5582 - val_accuracy: 0.9227 - val_loss: 0.2534 - learning_rate: 2.5000e-05\n",
      "Epoch 94/100\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8238 - loss: 0.5798\n",
      "Epoch 94: val_loss did not improve from 0.24729\n",
      "\n",
      "Epoch 94: val_accuracy improved from 0.92270 to 0.92845, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m758s\u001b[0m 2s/step - accuracy: 0.8238 - loss: 0.5798 - val_accuracy: 0.9285 - val_loss: 0.2477 - learning_rate: 2.5000e-05\n",
      "Epoch 95/100\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8175 - loss: 0.5790\n",
      "Epoch 95: val_loss did not improve from 0.24729\n",
      "\n",
      "Epoch 95: val_accuracy did not improve from 0.92845\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 2s/step - accuracy: 0.8175 - loss: 0.5790 - val_accuracy: 0.9243 - val_loss: 0.2527 - learning_rate: 2.5000e-05\n",
      "Epoch 96/100\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8298 - loss: 0.5597\n",
      "Epoch 96: val_loss improved from 0.24729 to 0.24021, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 96: val_accuracy improved from 0.92845 to 0.93174, saving model to bestaccuracy_transfer_finetune_model.keras\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m738s\u001b[0m 2s/step - accuracy: 0.8298 - loss: 0.5597 - val_accuracy: 0.9317 - val_loss: 0.2402 - learning_rate: 2.5000e-05\n",
      "Epoch 97/100\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8206 - loss: 0.5703\n",
      "Epoch 97: val_loss improved from 0.24021 to 0.23928, saving model to best_transfer_finetune_model.keras\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.93174\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m774s\u001b[0m 2s/step - accuracy: 0.8206 - loss: 0.5703 - val_accuracy: 0.9264 - val_loss: 0.2393 - learning_rate: 2.5000e-05\n",
      "Epoch 98/100\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8266 - loss: 0.5606\n",
      "Epoch 98: val_loss did not improve from 0.23928\n",
      "\n",
      "Epoch 98: val_accuracy did not improve from 0.93174\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 2s/step - accuracy: 0.8266 - loss: 0.5606 - val_accuracy: 0.9301 - val_loss: 0.2471 - learning_rate: 2.5000e-05\n",
      "Epoch 99/100\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8313 - loss: 0.5420\n",
      "Epoch 99: val_loss did not improve from 0.23928\n",
      "\n",
      "Epoch 99: val_accuracy did not improve from 0.93174\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 1s/step - accuracy: 0.8313 - loss: 0.5421 - val_accuracy: 0.9289 - val_loss: 0.2467 - learning_rate: 2.5000e-05\n",
      "Epoch 100/100\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8199 - loss: 0.5638\n",
      "Epoch 100: val_loss did not improve from 0.23928\n",
      "\n",
      "Epoch 100: val_accuracy did not improve from 0.93174\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 1s/step - accuracy: 0.8199 - loss: 0.5638 - val_accuracy: 0.9273 - val_loss: 0.2616 - learning_rate: 2.5000e-05\n",
      "Restoring model weights from the end of the best epoch: 97.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the previously saved model\n",
    "trmodel = load_model('bestaccuracy_transfer_finetune_model.keras')\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_transfer_finetune_model.keras', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "model_checkpoint2 = ModelCheckpoint('bestaccuracy_transfer_finetune_model.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "# Continue training the model\n",
    "history = trmodel.fit(\n",
    "    normalized_train,  # Your training dataset\n",
    "    epochs=100,\n",
    "    initial_epoch=90,# Continue training for 10 more epochs\n",
    "    validation_data=normalized_validation,  # Your validation dataset\n",
    "    callbacks=[early_stopping, model_checkpoint, model_checkpoint2, lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23534b4-4f5f-46ce-b8c0-2dec7e4c1911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
